{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread, imresize\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open(\"train.csv\").readlines())\n",
    "val_doc = np.random.permutation(open(\"val.csv\").readlines())\n",
    "batch_size = 30  #experiment with the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(663, 100)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_doc), len(val_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = 30 # number of frames\n",
    "# y = 120 # image width\n",
    "# z = 120 # image height\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(0,x)] #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    resize = imresize(image,(y,z))\n",
    "                    temp = resize/255 #Normalize data\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2]) #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            print(\"Batch: \",num_batches+1,\"Index:\", batch_size)\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    temp = imresize(image,(y,z))\n",
    "                    temp = temp/255 #Normalize data\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0])\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1])\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2])\n",
    "                   \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 10\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'train'\n",
    "val_path = 'val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 10  # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout\n",
    "\n",
    "#write your model here\n",
    "\n",
    "# Experiment -1 , Convolutional layers -2 , Dense layers -2 , Batch Normalization after every conv layer.\n",
    "#                  Dropout after conv and Dense layers\n",
    "\n",
    "model_1 = Sequential()\n",
    "model_1.add(Conv3D(filters=32,kernel_size=(3,3,3), padding='same', input_shape = (30, 120, 120, 3)))\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(BatchNormalization())\n",
    "model_1.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "model_1.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model_1.add(Conv3D(filters=16,kernel_size=(3,3,3), padding='same'))\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(BatchNormalization())\n",
    "model_1.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "model_1.add(Dropout(0.25))\n",
    "\n",
    "# flatten the output from conv layers and feed in to Dense layers\n",
    "model_1.add(Flatten())\n",
    "model_1.add(Dense(units=256,activation='relu'))\n",
    "model_1.add(Dropout(0.50))\n",
    "\n",
    "\n",
    "# final softmax layer\n",
    "\n",
    "model_1.add(Dense(units=5, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_3 (Conv3D)            (None, 30, 120, 120, 8)   656       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 30, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 30, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 15, 60, 60, 8)     0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 15, 60, 60, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 15, 60, 60, 16)    3472      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 15, 60, 60, 16)    64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 7, 30, 30, 16)     0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 7, 30, 30, 16)     0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 100800)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               25805056  \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 25,810,565\n",
      "Trainable params: 25,810,517\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = optimizers.Adam(lr=0.001) #write your optimizer\n",
    "model_1.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1) # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  val ; batch size = 40\n",
      "Source path =  train ; batch size = 40\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2/17 [==>...........................] - ETA: 1:32 - loss: 8.7216 - categorical_accuracy: 0.1625Batch:  3 Index: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/17 [=========================>....] - ETA: 9s - loss: 11.8822 - categorical_accuracy: 0.2200 Batch:  17 Index: 40\n",
      "17/17 [==============================] - 83s 5s/step - loss: 11.9127 - categorical_accuracy: 0.2231 - val_loss: 13.2168 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-05-2414_16_02.324946/model-00001-11.89575-0.22323-13.21684-0.18000.h5\n",
      "Epoch 2/10\n",
      "17/17 [==============================] - 43s 3s/step - loss: 11.7275 - categorical_accuracy: 0.2685 - val_loss: 12.8945 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-05-2414_16_02.324946/model-00002-11.72750-0.26854-12.89448-0.20000.h5\n",
      "Epoch 3/10\n",
      "10/17 [================>.............] - ETA: 17s - loss: 11.6331 - categorical_accuracy: 0.2783Batch:  29 Index: 23\n",
      "17/17 [==============================] - 41s 2s/step - loss: 11.8095 - categorical_accuracy: 0.2673 - val_loss: 12.0886 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-05-2414_16_02.324946/model-00003-11.81411-0.26703-12.08857-0.25000.h5\n",
      "Epoch 4/10\n",
      "17/17 [==============================] - 38s 2s/step - loss: 12.1287 - categorical_accuracy: 0.2446 - val_loss: 11.4356 - val_categorical_accuracy: 0.2833\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-05-2414_16_02.324946/model-00004-12.12869-0.24458-11.43561-0.28333.h5\n",
      "Epoch 5/10\n",
      "11/17 [==================>...........] - ETA: 12s - loss: 11.5464 - categorical_accuracy: 0.2823Batch:  35 Index: 19\n",
      "17/17 [==============================] - 37s 2s/step - loss: 11.3164 - categorical_accuracy: 0.2970 - val_loss: 12.4776 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-05-2414_16_02.324946/model-00005-11.31461-0.29712-12.47765-0.20000.h5\n",
      "Epoch 6/10\n",
      "17/17 [==============================] - 35s 2s/step - loss: 11.0986 - categorical_accuracy: 0.3114 - val_loss: 11.8443 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-05-2414_16_02.324946/model-00006-11.09862-0.31142-11.84434-0.25000.h5\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 7/10\n",
      "17/17 [==============================] - 33s 2s/step - loss: 11.2570 - categorical_accuracy: 0.2976 - val_loss: 11.2827 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-05-2414_16_02.324946/model-00007-11.25702-0.29758-11.28267-0.30000.h5\n",
      "Epoch 8/10\n",
      "17/17 [==============================] - 34s 2s/step - loss: 11.1398 - categorical_accuracy: 0.3080 - val_loss: 13.7533 - val_categorical_accuracy: 0.1333\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-05-2414_16_02.324946/model-00008-11.13983-0.30796-13.75328-0.13333.h5\n",
      "Epoch 9/10\n",
      "17/17 [==============================] - 33s 2s/step - loss: 10.8891 - categorical_accuracy: 0.3218 - val_loss: 12.4751 - val_categorical_accuracy: 0.2167\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-05-2414_16_02.324946/model-00009-10.88914-0.32180-12.47509-0.21667.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 10/10\n",
      "17/17 [==============================] - 32s 2s/step - loss: 10.7484 - categorical_accuracy: 0.3287 - val_loss: 11.5513 - val_categorical_accuracy: 0.2833\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-05-2414_16_02.324946/model-00010-10.74835-0.32872-11.55130-0.28333.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2050084400>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss: 10.7484 - categorical_accuracy: 0.3287 - val_loss: 11.5513 - val_categorical_accuracy: 0.2833\n",
    "#### model-00010-10.74835-0.32872-11.55130-0.28333.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 30 # number of frames\n",
    "y = 60 # image width\n",
    "z = 60 # image height\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(0,x)] #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    resize = imresize(image,(60,60))\n",
    "                    temp = resize/255 #Normalize data\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2]) #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            print(\"Batch: \",num_batches+1,\"Index:\", batch_size)\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    temp = imresize(image,(60,60))\n",
    "                    temp = temp/255 #Normalize data\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0])\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1])\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2])\n",
    "                   \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout\n",
    "\n",
    "#write your model here\n",
    "\n",
    "# Experiment -2 , Convolutional layers -2 , Dense layers -2 , Batch Normalization after every conv layer.\n",
    "#                  Dropout after conv and Dense layers , number of feature maps increased from 8 to 32 and 16 to 64,\n",
    "#                 Dense Layer unit size changed from 256 to 512 , changed dimensions of image to (60,60,3)\n",
    "\n",
    "model_2 = Sequential()\n",
    "model_2.add(Conv3D(filters=8,kernel_size=(3,3,3), padding='same', input_shape = (30, 60, 60, 3)))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(BatchNormalization())\n",
    "model_2.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "model_2.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model_2.add(Conv3D(filters=16,kernel_size=(3,3,3), padding='same'))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(BatchNormalization())\n",
    "model_2.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "model_2.add(Dropout(0.25))\n",
    "\n",
    "model_2.add(Conv3D(filters=32,kernel_size=(3,3,3), padding='same'))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(BatchNormalization())\n",
    "model_2.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "model_2.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "\n",
    "# flatten the output from conv layers and feed in to Dense layers\n",
    "model_2.add(Flatten())\n",
    "\n",
    "model_2.add(Dense(units=100,activation='relu'))\n",
    "model_2.add(Dropout(0.50))\n",
    "\n",
    "\n",
    "# final softmax layer\n",
    "\n",
    "model_2.add(Dense(units=5, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_19 (Conv3D)           (None, 30, 60, 60, 8)     656       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 30, 60, 60, 8)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 30, 60, 60, 8)     32        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_19 (MaxPooling (None, 15, 30, 30, 8)     0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 15, 30, 30, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_20 (Conv3D)           (None, 15, 30, 30, 16)    3472      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 15, 30, 30, 16)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 15, 30, 30, 16)    64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_20 (MaxPooling (None, 7, 15, 15, 16)     0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 7, 15, 15, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_21 (Conv3D)           (None, 7, 15, 15, 32)     13856     \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 7, 15, 15, 32)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 7, 15, 15, 32)     128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_21 (MaxPooling (None, 3, 7, 7, 32)       0         \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 3, 7, 7, 32)       0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 4704)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 100)               470500    \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 489,213\n",
      "Trainable params: 489,101\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = optimizers.Adam(lr=0.001) #write your optimizer\n",
    "model_2.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  val ; batch size = 30\n",
      "Source path =  train ; batch size = 30\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3/23 [==>...........................] - ETA: 1:12 - loss: 5.6347 - categorical_accuracy: 0.2333Batch:  4 Index: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/23 [==========================>...] - ETA: 6s - loss: 2.5851 - categorical_accuracy: 0.2651Batch:  23 Index: 30\n",
      "23/23 [==============================] - 69s 3s/step - loss: 2.5032 - categorical_accuracy: 0.2526 - val_loss: 1.3675 - val_categorical_accuracy: 0.4700\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-05-2414_16_02.324946/model-00001-2.53505-0.26244-1.36747-0.47000.h5\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 6s 246ms/step - loss: 1.5883 - categorical_accuracy: 0.2174 - val_loss: 1.5251 - val_categorical_accuracy: 0.4250\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-05-2414_16_02.324946/model-00002-1.58833-0.21739-1.52511-0.42500.h5\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 8s 349ms/step - loss: 1.5737 - categorical_accuracy: 0.2754 - val_loss: 2.2311 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-05-2414_16_02.324946/model-00003-1.57369-0.27536-2.23110-0.25000.h5\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 7s 300ms/step - loss: 1.4650 - categorical_accuracy: 0.3623 - val_loss: 4.0258 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-05-2414_16_02.324946/model-00004-1.46502-0.36232-4.02577-0.30000.h5\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 8s 365ms/step - loss: 1.6365 - categorical_accuracy: 0.2754 - val_loss: 6.2973 - val_categorical_accuracy: 0.1500\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-05-2414_16_02.324946/model-00005-1.63653-0.27536-6.29732-0.15000.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 7s 306ms/step - loss: 1.5684 - categorical_accuracy: 0.2754 - val_loss: 5.3157 - val_categorical_accuracy: 0.2250\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-05-2414_16_02.324946/model-00006-1.56842-0.27536-5.31573-0.22500.h5\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 8s 347ms/step - loss: 1.5655 - categorical_accuracy: 0.2609 - val_loss: 5.9873 - val_categorical_accuracy: 0.1750\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-05-2414_16_02.324946/model-00007-1.56550-0.26087-5.98731-0.17500.h5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 8s 329ms/step - loss: 1.5234 - categorical_accuracy: 0.2319 - val_loss: 4.0265 - val_categorical_accuracy: 0.2250\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-05-2414_16_02.324946/model-00008-1.52343-0.23188-4.02654-0.22500.h5\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 8s 363ms/step - loss: 1.5494 - categorical_accuracy: 0.2174 - val_loss: 3.7638 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-05-2414_16_02.324946/model-00009-1.54941-0.21739-3.76379-0.25000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 7s 311ms/step - loss: 1.4384 - categorical_accuracy: 0.2899 - val_loss: 2.7671 - val_categorical_accuracy: 0.2750\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-05-2414_16_02.324946/model-00010-1.43843-0.28986-2.76713-0.27500.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2039647828>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 30 # number of frames\n",
    "y = 60 # image width\n",
    "z = 60 # image height\n",
    "batch_size = 20\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(0,x)] #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    resize = imresize(image,(60,60))\n",
    "                    temp = resize/255 #Normalize data\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2]) #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            print(\"Batch: \",num_batches+1,\"Index:\", batch_size)\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    temp = imresize(image,(60,60))\n",
    "                    temp = temp/255 #Normalize data\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0])\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1])\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2])\n",
    "                   \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 10\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'train'\n",
    "val_path = 'val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 10  # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Experiment -3 , Changing batch size to 20 and removing BN , addition of conv and \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout\n",
    "\n",
    "x = 30 # number of frames\n",
    "y = 60 # image width\n",
    "z = 60 # image height\n",
    "\n",
    "model_3 = Sequential()\n",
    "model_3.add(Conv3D(32, kernel_size=(3, 3, 3), input_shape=(x,y,z,3), padding='same'))\n",
    "model_3.add(Activation('relu'))\n",
    "model_3.add(Conv3D(32, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_3.add(Activation('relu'))\n",
    "model_3.add(MaxPooling3D(pool_size=(3, 3, 3), padding='same'))\n",
    "model_3.add(Dropout(0.25))\n",
    "\n",
    "model_3.add(Conv3D(64, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_3.add(Activation('relu'))\n",
    "model_3.add(Conv3D(64, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_3.add(Activation('relu'))\n",
    "model_3.add(MaxPooling3D(pool_size=(3, 3, 3), padding='same'))\n",
    "model_3.add(Dropout(0.25))\n",
    "\n",
    "model_3.add(Flatten())\n",
    "model_3.add(Dense(512, activation='relu'))\n",
    "model_3.add(Dropout(0.5))\n",
    "model_3.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_34 (Conv3D)           (None, 30, 60, 60, 32)    2624      \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 30, 60, 60, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_35 (Conv3D)           (None, 30, 60, 60, 32)    27680     \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 30, 60, 60, 32)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_28 (MaxPooling (None, 10, 20, 20, 32)    0         \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 10, 20, 20, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_36 (Conv3D)           (None, 10, 20, 20, 64)    55360     \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 10, 20, 20, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_37 (Conv3D)           (None, 10, 20, 20, 64)    110656    \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 10, 20, 20, 64)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_29 (MaxPooling (None, 4, 7, 7, 64)       0         \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 4, 7, 7, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 512)               6423040   \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 6,621,925\n",
      "Trainable params: 6,621,925\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = optimizers.Adam(lr=0.001) #write your optimizer\n",
    "model_3.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  val ; batch size = 20\n",
      "Source path =  train ; batch size = 20\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/34 [===========================>..] - ETA: 4s - loss: 1.6531 - categorical_accuracy: 0.1859Batch:  34 Index: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:43: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:47: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 74s 2s/step - loss: 1.6662 - categorical_accuracy: 0.1840 - val_loss: 1.5121 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-05-2414_16_02.324946/model-00001-1.64903-0.18854-1.51206-0.23000.h5\n",
      "Epoch 2/10\n",
      "34/34 [==============================] - 9s 276ms/step - loss: 1.6048 - categorical_accuracy: 0.1765 - val_loss: 1.6112 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-05-2414_16_02.324946/model-00002-1.60482-0.17647-1.61125-0.21000.h5\n",
      "Epoch 3/10\n",
      "34/34 [==============================] - 14s 397ms/step - loss: 1.6133 - categorical_accuracy: 0.1667 - val_loss: 1.6113 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-05-2414_16_02.324946/model-00003-1.61334-0.16667-1.61134-0.21000.h5\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 4/10\n",
      "34/34 [==============================] - 11s 330ms/step - loss: 1.6136 - categorical_accuracy: 0.1765 - val_loss: 1.6108 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-05-2414_16_02.324946/model-00004-1.61356-0.17647-1.61078-0.21000.h5\n",
      "Epoch 5/10\n",
      "34/34 [==============================] - 14s 413ms/step - loss: 1.6112 - categorical_accuracy: 0.1667 - val_loss: 1.6108 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-05-2414_16_02.324946/model-00005-1.61124-0.16667-1.61084-0.21000.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 6/10\n",
      "34/34 [==============================] - 13s 382ms/step - loss: 1.6130 - categorical_accuracy: 0.1667 - val_loss: 1.6105 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-05-2414_16_02.324946/model-00006-1.61296-0.16667-1.61047-0.21000.h5\n",
      "Epoch 7/10\n",
      "34/34 [==============================] - 14s 398ms/step - loss: 1.6104 - categorical_accuracy: 0.2549 - val_loss: 1.6102 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-05-2414_16_02.324946/model-00007-1.61036-0.25490-1.61021-0.21000.h5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 8/10\n",
      "34/34 [==============================] - 13s 376ms/step - loss: 1.6068 - categorical_accuracy: 0.2255 - val_loss: 1.6102 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-05-2414_16_02.324946/model-00008-1.60680-0.22549-1.61018-0.21000.h5\n",
      "Epoch 9/10\n",
      "34/34 [==============================] - 12s 359ms/step - loss: 1.6114 - categorical_accuracy: 0.1373 - val_loss: 1.6103 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-05-2414_16_02.324946/model-00009-1.61137-0.13725-1.61026-0.21000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 10/10\n",
      "34/34 [==============================] - 13s 397ms/step - loss: 1.6114 - categorical_accuracy: 0.1863 - val_loss: 1.6102 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-05-2414_16_02.324946/model-00010-1.61141-0.18627-1.61021-0.21000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f20381a4208>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_3 - change the batch size to 20\n",
    "\n",
    "train_generator = generator(train_path, train_doc, 20)\n",
    "val_generator = generator(val_path, val_doc, 20)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  val ; batch size = 20\n",
      "Source path =  train ; batch size = 20\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/34 [===========================>..] - ETA: 4s - loss: 1.6096 - categorical_accuracy: 0.2078Batch:  34 Index: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:43: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:47: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 71s 2s/step - loss: 1.6098 - categorical_accuracy: 0.2097 - val_loss: 1.6101 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-05-2414_16_02.324946/model-00001-1.60974-0.20664-1.61012-0.21000.h5\n",
      "Epoch 2/10\n",
      "34/34 [==============================] - 12s 356ms/step - loss: 1.6056 - categorical_accuracy: 0.3137 - val_loss: 1.6100 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-05-2414_16_02.324946/model-00002-1.60558-0.31373-1.60995-0.21000.h5\n",
      "Epoch 3/10\n",
      "34/34 [==============================] - 13s 368ms/step - loss: 1.6087 - categorical_accuracy: 0.1961 - val_loss: 1.6089 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-05-2414_16_02.324946/model-00003-1.60869-0.19608-1.60889-0.21000.h5\n",
      "Epoch 4/10\n",
      "34/34 [==============================] - 12s 366ms/step - loss: 1.5937 - categorical_accuracy: 0.2059 - val_loss: 1.5878 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-05-2414_16_02.324946/model-00004-1.59366-0.20588-1.58784-0.18000.h5\n",
      "Epoch 5/10\n",
      "34/34 [==============================] - 13s 391ms/step - loss: 1.6053 - categorical_accuracy: 0.1863 - val_loss: 1.5788 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-05-2414_16_02.324946/model-00005-1.60531-0.18627-1.57882-0.30000.h5\n",
      "Epoch 6/10\n",
      "34/34 [==============================] - 12s 365ms/step - loss: 1.5569 - categorical_accuracy: 0.2647 - val_loss: 1.5200 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-05-2414_16_02.324946/model-00006-1.55687-0.26471-1.51998-0.21000.h5\n",
      "Epoch 7/10\n",
      "34/34 [==============================] - 14s 415ms/step - loss: 1.4451 - categorical_accuracy: 0.3039 - val_loss: 1.4872 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-05-2414_16_02.324946/model-00007-1.44513-0.30392-1.48719-0.24000.h5\n",
      "Epoch 8/10\n",
      "34/34 [==============================] - 14s 413ms/step - loss: 1.5426 - categorical_accuracy: 0.2745 - val_loss: 1.4940 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-05-2414_16_02.324946/model-00008-1.54258-0.27451-1.49395-0.27000.h5\n",
      "Epoch 9/10\n",
      "34/34 [==============================] - 13s 369ms/step - loss: 1.4249 - categorical_accuracy: 0.3529 - val_loss: 1.4091 - val_categorical_accuracy: 0.3600\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-05-2414_16_02.324946/model-00009-1.42490-0.35294-1.40910-0.36000.h5\n",
      "Epoch 10/10\n",
      "34/34 [==============================] - 12s 360ms/step - loss: 1.4722 - categorical_accuracy: 0.3039 - val_loss: 1.4157 - val_categorical_accuracy: 0.3300\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-05-2414_16_02.324946/model-00010-1.47221-0.30392-1.41566-0.33000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f20381a4898>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 15 # number of frames\n",
    "y = 60 # image width\n",
    "z = 60 # image height\n",
    "batch_size = 20\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(0,x)] #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    resize = imresize(image,(90,90))\n",
    "                    temp = resize/255 #Normalize data\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2]) #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            print(\"Batch: \",num_batches+1,\"Index:\", batch_size)\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    temp = imresize(image,(90,90))\n",
    "                    temp = temp/255 #Normalize data\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0])\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1])\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2])\n",
    "                   \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment -4 , Changing batch size to 20 and removing BN , addition of conv and \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout\n",
    "\n",
    "x = 15 # number of frames\n",
    "y = 60 # image width\n",
    "z = 60 # image height\n",
    "\n",
    "model_4 = Sequential()\n",
    "model_4.add(Conv3D(64, kernel_size=(3, 3, 3), input_shape=(x,y,z,3), padding='same'))\n",
    "model_4.add(Activation('relu'))\n",
    "model_4.add(Conv3D(64, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_4.add(Activation('relu'))\n",
    "model_4.add(MaxPooling3D(pool_size=(3, 3, 3), padding='same'))\n",
    "model_4.add(Dropout(0.25))\n",
    "\n",
    "model_4.add(Conv3D(32, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_4.add(Activation('relu'))\n",
    "model_4.add(MaxPooling3D(pool_size=(3, 3, 3), padding='same'))\n",
    "model_4.add(Dropout(0.25))\n",
    "\n",
    "model_4.add(Flatten())\n",
    "model_4.add(Dense(512, activation='relu'))\n",
    "model_4.add(Dropout(0.5))\n",
    "model_4.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_4 (Conv3D)            (None, 15, 60, 60, 64)    5248      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 15, 60, 60, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 15, 60, 60, 64)    110656    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 15, 60, 60, 64)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 5, 20, 20, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 5, 20, 20, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_6 (Conv3D)            (None, 5, 20, 20, 32)     55328     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 5, 20, 20, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 2, 7, 7, 32)       0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 2, 7, 7, 32)       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 1,779,941\n",
      "Trainable params: 1,779,941\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = optimizers.Adam(lr=0.001) #write your optimizer\n",
    "model_4.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing batch size to 30 \n",
    "\n",
    "x = 15 # number of frames\n",
    "y = 90 # image width\n",
    "z = 90 # image height\n",
    "batch_size = 30\n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1) # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  val ; batch size = 30\n",
      "Source path =  train ; batch size = 30\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3/23 [==>...........................] - ETA: 1:40 - loss: 1.8006 - categorical_accuracy: 0.1889Batch:  4 Index: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/23 [==========================>...] - ETA: 9s - loss: 1.6347 - categorical_accuracy: 0.2206 Batch:  23 Index: 30\n",
      "23/23 [==============================] - 107s 5s/step - loss: 1.6302 - categorical_accuracy: 0.2120 - val_loss: 1.5711 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-05-2417_26_11.161846/model-00001-1.63165-0.22021-1.57112-0.29000.h5\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 1.5756 - categorical_accuracy: 0.2899 - val_loss: 1.4371 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-05-2417_26_11.161846/model-00002-1.57563-0.28986-1.43710-0.25000.h5\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 1.5141 - categorical_accuracy: 0.3043 - val_loss: 1.6123 - val_categorical_accuracy: 0.2250\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-05-2417_26_11.161846/model-00003-1.51413-0.30435-1.61228-0.22500.h5\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 4s 183ms/step - loss: 1.5860 - categorical_accuracy: 0.2609 - val_loss: 1.6625 - val_categorical_accuracy: 0.1250\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-05-2417_26_11.161846/model-00004-1.58605-0.26087-1.66245-0.12500.h5\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 4s 188ms/step - loss: 1.6139 - categorical_accuracy: 0.2609 - val_loss: 1.6194 - val_categorical_accuracy: 0.2250\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-05-2417_26_11.161846/model-00005-1.61388-0.26087-1.61939-0.22500.h5\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 5s 200ms/step - loss: 1.5718 - categorical_accuracy: 0.2899 - val_loss: 1.6310 - val_categorical_accuracy: 0.1000\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-05-2417_26_11.161846/model-00006-1.57184-0.28986-1.63105-0.10000.h5\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 4s 185ms/step - loss: 1.5758 - categorical_accuracy: 0.1884 - val_loss: 1.4956 - val_categorical_accuracy: 0.3750\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-05-2417_26_11.161846/model-00007-1.57577-0.18841-1.49558-0.37500.h5\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 1.5414 - categorical_accuracy: 0.2609 - val_loss: 1.5432 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-05-2417_26_11.161846/model-00008-1.54143-0.26087-1.54321-0.30000.h5\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 5s 200ms/step - loss: 1.5536 - categorical_accuracy: 0.2899 - val_loss: 1.4381 - val_categorical_accuracy: 0.5250\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-05-2417_26_11.161846/model-00009-1.55365-0.28986-1.43813-0.52500.h5\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 4s 184ms/step - loss: 1.4475 - categorical_accuracy: 0.3043 - val_loss: 1.3992 - val_categorical_accuracy: 0.4250\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-05-2417_26_11.161846/model-00010-1.44748-0.30435-1.39925-0.42500.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdf60208390>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout\n",
    "\n",
    "x = 20 # number of frames\n",
    "y = 60 # image width\n",
    "z = 60 # image height\n",
    "\n",
    "model_5 = Sequential()\n",
    "model_5.add(Conv3D(64, kernel_size=(3, 3, 3), input_shape=(x,y,z,3), padding='same'))\n",
    "model_5.add(Activation('relu'))\n",
    "model_5.add(Conv3D(64, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_5.add(Activation('relu'))\n",
    "model_5.add(MaxPooling3D(pool_size=(3, 3, 3), padding='same'))\n",
    "model_5.add(Dropout(0.1))\n",
    "\n",
    "model_5.add(Conv3D(64, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_5.add(Activation('relu'))\n",
    "model_5.add(MaxPooling3D(pool_size=(3, 3, 3), padding='same'))\n",
    "model_5.add(Dropout(0.1))\n",
    "\n",
    "model_5.add(Conv3D(64, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_5.add(Activation('relu'))\n",
    "model_5.add(MaxPooling3D(pool_size=(3, 3, 3), padding='same'))\n",
    "model_5.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "model_5.add(Flatten())\n",
    "model_5.add(Dense(512, activation='relu'))\n",
    "model_5.add(Dropout(0.5))\n",
    "\n",
    "model_5.add(Dense(512, activation='relu'))\n",
    "model_5.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model_5.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_15 (Conv3D)           (None, 20, 60, 60, 64)    5248      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 20, 60, 60, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_16 (Conv3D)           (None, 20, 60, 60, 64)    110656    \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 20, 60, 60, 64)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_11 (MaxPooling (None, 7, 20, 20, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 7, 20, 20, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_17 (Conv3D)           (None, 7, 20, 20, 64)     110656    \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 7, 20, 20, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_12 (MaxPooling (None, 3, 7, 7, 64)       0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 3, 7, 7, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv3d_18 (Conv3D)           (None, 3, 7, 7, 64)       110656    \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 3, 7, 7, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_13 (MaxPooling (None, 1, 3, 3, 64)       0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 1, 3, 3, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 512)               295424    \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 897,861\n",
      "Trainable params: 897,861\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = optimizers.Adam(lr=0.001) #write your optimizer\n",
    "model_5.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_5.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  val ; batch size = 30\n",
      "Source path =  train ; batch size = 30\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3/23 [==>...........................] - ETA: 1:56 - loss: 1.6196 - categorical_accuracy: 0.2000Batch:  4 Index: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/23 [==========================>...] - ETA: 10s - loss: 1.6161 - categorical_accuracy: 0.2159Batch:  23 Index: 30\n",
      "23/23 [==============================] - 118s 5s/step - loss: 1.6102 - categorical_accuracy: 0.2230 - val_loss: 1.6148 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-05-2417_26_11.161846/model-00001-1.61315-0.21870-1.61480-0.25000.h5\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 6s 241ms/step - loss: 1.6183 - categorical_accuracy: 0.3333 - val_loss: 1.5900 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-05-2417_26_11.161846/model-00002-1.61826-0.33333-1.58999-0.20000.h5\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 6s 240ms/step - loss: 1.6180 - categorical_accuracy: 0.1739 - val_loss: 1.6021 - val_categorical_accuracy: 0.2250\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-05-2417_26_11.161846/model-00003-1.61801-0.17391-1.60206-0.22500.h5\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 6s 240ms/step - loss: 1.6215 - categorical_accuracy: 0.2319 - val_loss: 1.5674 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-05-2417_26_11.161846/model-00004-1.62155-0.23188-1.56742-0.25000.h5\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 6s 256ms/step - loss: 1.6071 - categorical_accuracy: 0.1304 - val_loss: 1.5979 - val_categorical_accuracy: 0.1000\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-05-2417_26_11.161846/model-00005-1.60710-0.13043-1.59795-0.10000.h5\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 7s 290ms/step - loss: 1.5873 - categorical_accuracy: 0.2319 - val_loss: 1.5096 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-05-2417_26_11.161846/model-00006-1.58729-0.23188-1.50965-0.35000.h5\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 6s 254ms/step - loss: 1.4937 - categorical_accuracy: 0.2174 - val_loss: 1.4311 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-05-2417_26_11.161846/model-00007-1.49370-0.21739-1.43107-0.35000.h5\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 6s 253ms/step - loss: 1.6509 - categorical_accuracy: 0.3623 - val_loss: 1.5626 - val_categorical_accuracy: 0.3250\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-05-2417_26_11.161846/model-00008-1.65091-0.36232-1.56264-0.32500.h5\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 6s 257ms/step - loss: 1.6052 - categorical_accuracy: 0.1739 - val_loss: 1.5853 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-05-2417_26_11.161846/model-00009-1.60523-0.17391-1.58531-0.20000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 5s 238ms/step - loss: 1.5668 - categorical_accuracy: 0.2754 - val_loss: 1.4717 - val_categorical_accuracy: 0.4250\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-05-2417_26_11.161846/model-00010-1.56677-0.27536-1.47169-0.42500.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdf6eaa52e8>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout\n",
    "\n",
    "x = 25 # number of frames\n",
    "y = 90 # image width\n",
    "z = 90 # image height\n",
    "\n",
    "model_5 = Sequential()\n",
    "model_5.add(Conv3D(64, kernel_size=(3, 3, 3), input_shape=(x,y,z,3), padding='same'))\n",
    "model_5.add(Activation('relu'))\n",
    "model_5.add(Conv3D(64, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_5.add(Activation('relu'))\n",
    "model_5.add(MaxPooling3D(pool_size=(3, 3, 3), padding='same'))\n",
    "model_5.add(Dropout(0.1))\n",
    "\n",
    "model_5.add(Conv3D(64, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_5.add(Activation('relu'))\n",
    "model_5.add(MaxPooling3D(pool_size=(3, 3, 3), padding='same'))\n",
    "model_5.add(Dropout(0.1))\n",
    "\n",
    "model_5.add(Conv3D(64, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_5.add(Activation('relu'))\n",
    "model_5.add(MaxPooling3D(pool_size=(3, 3, 3), padding='same'))\n",
    "model_5.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "model_5.add(Conv3D(64, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_5.add(Activation('relu'))\n",
    "model_5.add(MaxPooling3D(pool_size=(3, 3, 3), padding='same'))\n",
    "model_5.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "model_5.add(Flatten())\n",
    "model_5.add(Dense(1000, activation='relu'))\n",
    "model_5.add(Dropout(0.5))\n",
    "\n",
    "model_5.add(Dense(512, activation='relu'))\n",
    "model_5.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model_5.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_19 (Conv3D)           (None, 25, 90, 90, 64)    5248      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 25, 90, 90, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_20 (Conv3D)           (None, 25, 90, 90, 64)    110656    \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 25, 90, 90, 64)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_14 (MaxPooling (None, 9, 30, 30, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 9, 30, 30, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_21 (Conv3D)           (None, 9, 30, 30, 64)     110656    \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 9, 30, 30, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_15 (MaxPooling (None, 3, 10, 10, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 3, 10, 10, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_22 (Conv3D)           (None, 3, 10, 10, 64)     110656    \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 3, 10, 10, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_16 (MaxPooling (None, 1, 4, 4, 64)       0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 1, 4, 4, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv3d_23 (Conv3D)           (None, 1, 4, 4, 64)       110656    \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 1, 4, 4, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_17 (MaxPooling (None, 1, 2, 2, 64)       0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 1, 2, 2, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1000)              257000    \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 512)               512512    \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 1,219,949\n",
      "Trainable params: 1,219,949\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = optimizers.Adam(lr=0.001) #write your optimizer\n",
    "model_5.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_5.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "\n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3/23 [==>...........................] - ETA: 2:24 - loss: 1.5970 - categorical_accuracy: 0.2444Batch:  23 Index: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 44s 2s/step - loss: 1.6223 - categorical_accuracy: 0.2981 - val_loss: 1.6164 - val_categorical_accuracy: 0.1750\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-05-2417_26_11.161846/model-00001-1.60540-0.25424-1.61644-0.17500.h5\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 15s 649ms/step - loss: 1.6197 - categorical_accuracy: 0.1739 - val_loss: 1.6026 - val_categorical_accuracy: 0.2250\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-05-2417_26_11.161846/model-00002-1.61968-0.17391-1.60262-0.22500.h5\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 15s 648ms/step - loss: 1.6084 - categorical_accuracy: 0.1739 - val_loss: 1.6317 - val_categorical_accuracy: 0.0750\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-05-2417_26_11.161846/model-00003-1.60839-0.17391-1.63166-0.07500.h5\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 15s 652ms/step - loss: 1.6112 - categorical_accuracy: 0.2174 - val_loss: 1.5609 - val_categorical_accuracy: 0.4750\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-05-2417_26_11.161846/model-00004-1.61120-0.21739-1.56093-0.47500.h5\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 15s 651ms/step - loss: 1.5630 - categorical_accuracy: 0.2609 - val_loss: 1.4965 - val_categorical_accuracy: 0.4250\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-05-2417_26_11.161846/model-00005-1.56296-0.26087-1.49654-0.42500.h5\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 15s 649ms/step - loss: 1.6608 - categorical_accuracy: 0.1594 - val_loss: 1.5878 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-05-2417_26_11.161846/model-00006-1.66075-0.15942-1.58777-0.25000.h5\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 15s 651ms/step - loss: 1.7930 - categorical_accuracy: 0.2609 - val_loss: 1.9497 - val_categorical_accuracy: 0.3750\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-05-2417_26_11.161846/model-00007-1.79300-0.26087-1.94970-0.37500.h5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 15s 650ms/step - loss: 1.7028 - categorical_accuracy: 0.2319 - val_loss: 1.6012 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-05-2417_26_11.161846/model-00008-1.70284-0.23188-1.60123-0.25000.h5\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 15s 651ms/step - loss: 1.5671 - categorical_accuracy: 0.1884 - val_loss: 1.5595 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-05-2417_26_11.161846/model-00009-1.56712-0.18841-1.55950-0.20000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 15s 651ms/step - loss: 1.5789 - categorical_accuracy: 0.1884 - val_loss: 1.5673 - val_categorical_accuracy: 0.1500\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-05-2417_26_11.161846/model-00010-1.57891-0.18841-1.56732-0.15000.h5\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 15s 651ms/step - loss: 1.5290 - categorical_accuracy: 0.1884 - val_loss: 1.5355 - val_categorical_accuracy: 0.3250\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-05-2417_26_11.161846/model-00011-1.52899-0.18841-1.53554-0.32500.h5\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 15s 652ms/step - loss: 1.5469 - categorical_accuracy: 0.1594 - val_loss: 1.5155 - val_categorical_accuracy: 0.2250\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-05-2417_26_11.161846/model-00012-1.54689-0.15942-1.51551-0.22500.h5\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 15s 650ms/step - loss: 1.5151 - categorical_accuracy: 0.2464 - val_loss: 1.4480 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-05-2417_26_11.161846/model-00013-1.51509-0.24638-1.44796-0.45000.h5\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 15s 652ms/step - loss: 1.4926 - categorical_accuracy: 0.3478 - val_loss: 1.3958 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-05-2417_26_11.161846/model-00014-1.49259-0.34783-1.39585-0.25000.h5\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 15s 652ms/step - loss: 1.4117 - categorical_accuracy: 0.3188 - val_loss: 1.4475 - val_categorical_accuracy: 0.3250\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-05-2417_26_11.161846/model-00015-1.41169-0.31884-1.44746-0.32500.h5\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 15s 652ms/step - loss: 1.4338 - categorical_accuracy: 0.3478 - val_loss: 1.4399 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-05-2417_26_11.161846/model-00016-1.43385-0.34783-1.43993-0.40000.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 15s 652ms/step - loss: 1.3956 - categorical_accuracy: 0.4058 - val_loss: 1.5011 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-05-2417_26_11.161846/model-00017-1.39562-0.40580-1.50114-0.35000.h5\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 15s 651ms/step - loss: 1.4497 - categorical_accuracy: 0.3333 - val_loss: 1.3571 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-05-2417_26_11.161846/model-00018-1.44968-0.33333-1.35713-0.40000.h5\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 15s 651ms/step - loss: 1.5580 - categorical_accuracy: 0.3623 - val_loss: 1.3548 - val_categorical_accuracy: 0.3750\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-05-2417_26_11.161846/model-00019-1.55798-0.36232-1.35477-0.37500.h5\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 15s 651ms/step - loss: 1.4025 - categorical_accuracy: 0.4493 - val_loss: 1.3430 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-05-2417_26_11.161846/model-00020-1.40248-0.44928-1.34299-0.35000.h5\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 15s 650ms/step - loss: 1.3966 - categorical_accuracy: 0.3623 - val_loss: 1.3316 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00021: saving model to model_init_2020-05-2417_26_11.161846/model-00021-1.39657-0.36232-1.33163-0.50000.h5\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 15s 652ms/step - loss: 1.3986 - categorical_accuracy: 0.3333 - val_loss: 1.4347 - val_categorical_accuracy: 0.4250\n",
      "\n",
      "Epoch 00022: saving model to model_init_2020-05-2417_26_11.161846/model-00022-1.39863-0.33333-1.43467-0.42500.h5\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 15s 651ms/step - loss: 1.4201 - categorical_accuracy: 0.3043 - val_loss: 1.3161 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00023: saving model to model_init_2020-05-2417_26_11.161846/model-00023-1.42008-0.30435-1.31606-0.40000.h5\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 15s 651ms/step - loss: 1.4358 - categorical_accuracy: 0.3188 - val_loss: 1.2331 - val_categorical_accuracy: 0.3750\n",
      "\n",
      "Epoch 00024: saving model to model_init_2020-05-2417_26_11.161846/model-00024-1.43583-0.31884-1.23307-0.37500.h5\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 15s 650ms/step - loss: 1.4104 - categorical_accuracy: 0.3478 - val_loss: 1.3170 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00025: saving model to model_init_2020-05-2417_26_11.161846/model-00025-1.41038-0.34783-1.31703-0.45000.h5\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 15s 651ms/step - loss: 1.3671 - categorical_accuracy: 0.3768 - val_loss: 1.3616 - val_categorical_accuracy: 0.3250\n",
      "\n",
      "Epoch 00026: saving model to model_init_2020-05-2417_26_11.161846/model-00026-1.36705-0.37681-1.36159-0.32500.h5\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 15s 652ms/step - loss: 1.3700 - categorical_accuracy: 0.3333 - val_loss: 1.3059 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00027: saving model to model_init_2020-05-2417_26_11.161846/model-00027-1.36997-0.33333-1.30590-0.40000.h5\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 15s 652ms/step - loss: 1.3465 - categorical_accuracy: 0.3478 - val_loss: 1.3271 - val_categorical_accuracy: 0.4750\n",
      "\n",
      "Epoch 00028: saving model to model_init_2020-05-2417_26_11.161846/model-00028-1.34645-0.34783-1.32712-0.47500.h5\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 15s 652ms/step - loss: 1.4072 - categorical_accuracy: 0.3188 - val_loss: 1.3332 - val_categorical_accuracy: 0.2750\n",
      "\n",
      "Epoch 00029: saving model to model_init_2020-05-2417_26_11.161846/model-00029-1.40717-0.31884-1.33320-0.27500.h5\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 15s 651ms/step - loss: 1.4061 - categorical_accuracy: 0.3913 - val_loss: 1.2851 - val_categorical_accuracy: 0.5250\n",
      "\n",
      "Epoch 00030: saving model to model_init_2020-05-2417_26_11.161846/model-00030-1.40608-0.39130-1.28509-0.52500.h5\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 15s 653ms/step - loss: 1.4095 - categorical_accuracy: 0.3333 - val_loss: 1.3447 - val_categorical_accuracy: 0.3750\n",
      "\n",
      "Epoch 00031: saving model to model_init_2020-05-2417_26_11.161846/model-00031-1.40953-0.33333-1.34470-0.37500.h5\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 15s 652ms/step - loss: 1.3102 - categorical_accuracy: 0.3478 - val_loss: 1.3336 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00032: saving model to model_init_2020-05-2417_26_11.161846/model-00032-1.31016-0.34783-1.33361-0.50000.h5\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 15s 653ms/step - loss: 1.3567 - categorical_accuracy: 0.3333 - val_loss: 1.2980 - val_categorical_accuracy: 0.2750\n",
      "\n",
      "Epoch 00033: saving model to model_init_2020-05-2417_26_11.161846/model-00033-1.35669-0.33333-1.29800-0.27500.h5\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 15s 651ms/step - loss: 1.4104 - categorical_accuracy: 0.3043 - val_loss: 1.3116 - val_categorical_accuracy: 0.3250\n",
      "\n",
      "Epoch 00034: saving model to model_init_2020-05-2417_26_11.161846/model-00034-1.41042-0.30435-1.31157-0.32500.h5\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 15s 653ms/step - loss: 1.4105 - categorical_accuracy: 0.2899 - val_loss: 1.3217 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00035: saving model to model_init_2020-05-2417_26_11.161846/model-00035-1.41048-0.28986-1.32170-0.45000.h5\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 15s 652ms/step - loss: 1.3878 - categorical_accuracy: 0.3043 - val_loss: 1.3148 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00036: saving model to model_init_2020-05-2417_26_11.161846/model-00036-1.38778-0.30435-1.31476-0.40000.h5\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 15s 652ms/step - loss: 1.2806 - categorical_accuracy: 0.4203 - val_loss: 1.2603 - val_categorical_accuracy: 0.5250\n",
      "\n",
      "Epoch 00037: saving model to model_init_2020-05-2417_26_11.161846/model-00037-1.28059-0.42029-1.26031-0.52500.h5\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 15s 653ms/step - loss: 1.3967 - categorical_accuracy: 0.4493 - val_loss: 1.3381 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00038: saving model to model_init_2020-05-2417_26_11.161846/model-00038-1.39673-0.44928-1.33813-0.30000.h5\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 15s 652ms/step - loss: 1.3460 - categorical_accuracy: 0.4203 - val_loss: 1.3108 - val_categorical_accuracy: 0.3750\n",
      "\n",
      "Epoch 00039: saving model to model_init_2020-05-2417_26_11.161846/model-00039-1.34600-0.42029-1.31076-0.37500.h5\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 15s 653ms/step - loss: 1.3684 - categorical_accuracy: 0.3188 - val_loss: 1.4033 - val_categorical_accuracy: 0.3750\n",
      "\n",
      "Epoch 00040: saving model to model_init_2020-05-2417_26_11.161846/model-00040-1.36843-0.31884-1.40326-0.37500.h5\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 15s 652ms/step - loss: 1.3902 - categorical_accuracy: 0.3043 - val_loss: 1.2646 - val_categorical_accuracy: 0.4250\n",
      "\n",
      "Epoch 00041: saving model to model_init_2020-05-2417_26_11.161846/model-00041-1.39020-0.30435-1.26459-0.42500.h5\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 15s 653ms/step - loss: 1.3496 - categorical_accuracy: 0.3188 - val_loss: 1.3328 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00042: saving model to model_init_2020-05-2417_26_11.161846/model-00042-1.34960-0.31884-1.33277-0.35000.h5\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 15s 652ms/step - loss: 1.4411 - categorical_accuracy: 0.3768 - val_loss: 1.2973 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00043: saving model to model_init_2020-05-2417_26_11.161846/model-00043-1.44111-0.37681-1.29735-0.55000.h5\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 15s 652ms/step - loss: 1.3606 - categorical_accuracy: 0.3623 - val_loss: 1.3241 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00044: saving model to model_init_2020-05-2417_26_11.161846/model-00044-1.36055-0.36232-1.32407-0.25000.h5\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 15s 652ms/step - loss: 1.3241 - categorical_accuracy: 0.3768 - val_loss: 1.3606 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00045: saving model to model_init_2020-05-2417_26_11.161846/model-00045-1.32409-0.37681-1.36056-0.40000.h5\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 15s 651ms/step - loss: 1.3900 - categorical_accuracy: 0.4638 - val_loss: 1.2616 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00046: saving model to model_init_2020-05-2417_26_11.161846/model-00046-1.38998-0.46377-1.26158-0.45000.h5\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 15s 652ms/step - loss: 1.3748 - categorical_accuracy: 0.3478 - val_loss: 1.3520 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00047: saving model to model_init_2020-05-2417_26_11.161846/model-00047-1.37478-0.34783-1.35203-0.30000.h5\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 15s 651ms/step - loss: 1.3498 - categorical_accuracy: 0.3913 - val_loss: 1.2356 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00048: saving model to model_init_2020-05-2417_26_11.161846/model-00048-1.34983-0.39130-1.23557-0.50000.h5\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 15s 653ms/step - loss: 1.3631 - categorical_accuracy: 0.2899 - val_loss: 1.3664 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00049: saving model to model_init_2020-05-2417_26_11.161846/model-00049-1.36306-0.28986-1.36641-0.35000.h5\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 15s 651ms/step - loss: 1.3357 - categorical_accuracy: 0.3478 - val_loss: 1.3227 - val_categorical_accuracy: 0.3750\n",
      "\n",
      "Epoch 00050: saving model to model_init_2020-05-2417_26_11.161846/model-00050-1.33567-0.34783-1.32266-0.37500.h5\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdf6d38af60>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=50, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_36 (Conv3D)           (None, 25, 120, 120, 64)  5248      \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 25, 120, 120, 64)  0         \n",
      "_________________________________________________________________\n",
      "conv3d_37 (Conv3D)           (None, 25, 120, 120, 32)  55328     \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 25, 120, 120, 32)  0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_24 (MaxPooling (None, 9, 40, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 9, 40, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_38 (Conv3D)           (None, 9, 40, 40, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 9, 40, 40, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_39 (Conv3D)           (None, 9, 40, 40, 32)     55328     \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 9, 40, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_25 (MaxPooling (None, 3, 14, 14, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 3, 14, 14, 32)     0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 18816)             0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 512)               9634304   \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 9,938,181\n",
      "Trainable params: 9,938,181\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout\n",
    "\n",
    "x = 25 # number of frames\n",
    "y = 120 # image width\n",
    "z = 120 # image height\n",
    "\n",
    "model_6 = Sequential()\n",
    "model_6.add(Conv3D(64, kernel_size=(3, 3, 3), input_shape=(x,y,z,3), padding='same'))\n",
    "model_6.add(Activation('relu'))\n",
    "model_6.add(Conv3D(32, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_6.add(Activation('relu'))\n",
    "model_6.add(MaxPooling3D(pool_size=(3, 3, 3), padding='same'))\n",
    "model_6.add(Dropout(0.2))\n",
    "\n",
    "model_6.add(Conv3D(64, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_6.add(Activation('relu'))\n",
    "model_6.add(Conv3D(32, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_6.add(Activation('relu'))\n",
    "model_6.add(MaxPooling3D(pool_size=(3, 3, 3), padding='same'))\n",
    "model_6.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_6.add(Flatten())\n",
    "model_6.add(Dense(512, activation='relu'))\n",
    "model_6.add(Dropout(0.5))\n",
    "\n",
    "model_6.add(Dense(256, activation='relu'))\n",
    "model_6.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model_6.add(Dense(5, activation='softmax'))\n",
    "optimiser = optimizers.Adam(lr=0.001) #write your optimizer\n",
    "model_6.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_6.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "\n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  val ; batch size = 20\n",
      "Source path =  Epoch 1/20train ; batch size = \n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/34 [===================>..........] - ETA: 37s - loss: 1.6275 - categorical_accuracy: 0.1630Batch:  34 Index: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 118s 3s/step - loss: 1.6230 - categorical_accuracy: 0.1773 - val_loss: 1.6085 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-05-2417_26_11.161846/model-00001-1.62342-0.17345-1.60853-0.21000.h5\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 29s 846ms/step - loss: 1.6088 - categorical_accuracy: 0.1961 - val_loss: 1.6080 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-05-2417_26_11.161846/model-00002-1.60884-0.19608-1.60796-0.23000.h5\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 29s 847ms/step - loss: 1.6178 - categorical_accuracy: 0.2255 - val_loss: 1.6079 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-05-2417_26_11.161846/model-00003-1.61778-0.22549-1.60792-0.23000.h5\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 29s 847ms/step - loss: 1.6087 - categorical_accuracy: 0.2157 - val_loss: 1.6079 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-05-2417_26_11.161846/model-00004-1.60869-0.21569-1.60791-0.23000.h5\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 29s 847ms/step - loss: 1.6156 - categorical_accuracy: 0.1373 - val_loss: 1.6078 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-05-2417_26_11.161846/model-00005-1.61564-0.13725-1.60777-0.23000.h5\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 29s 849ms/step - loss: 1.6120 - categorical_accuracy: 0.2059 - val_loss: 1.6074 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-05-2417_26_11.161846/model-00006-1.61201-0.20588-1.60744-0.23000.h5\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 29s 848ms/step - loss: 1.6141 - categorical_accuracy: 0.1765 - val_loss: 1.6080 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-05-2417_26_11.161846/model-00007-1.61415-0.17647-1.60799-0.23000.h5\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 29s 847ms/step - loss: 1.6117 - categorical_accuracy: 0.1373 - val_loss: 1.6078 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-05-2417_26_11.161846/model-00008-1.61171-0.13725-1.60780-0.21000.h5\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 29s 845ms/step - loss: 1.6061 - categorical_accuracy: 0.2647 - val_loss: 1.6076 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-05-2417_26_11.161846/model-00009-1.60613-0.26471-1.60762-0.21000.h5\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 29s 848ms/step - loss: 1.6103 - categorical_accuracy: 0.2255 - val_loss: 1.6077 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-05-2417_26_11.161846/model-00010-1.61035-0.22549-1.60766-0.21000.h5\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 29s 848ms/step - loss: 1.6093 - categorical_accuracy: 0.2549 - val_loss: 1.6077 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-05-2417_26_11.161846/model-00011-1.60931-0.25490-1.60765-0.21000.h5\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - 29s 852ms/step - loss: 1.6086 - categorical_accuracy: 0.2353 - val_loss: 1.6075 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-05-2417_26_11.161846/model-00012-1.60859-0.23529-1.60749-0.21000.h5\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - 29s 847ms/step - loss: 1.6078 - categorical_accuracy: 0.2745 - val_loss: 1.6074 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-05-2417_26_11.161846/model-00013-1.60785-0.27451-1.60742-0.21000.h5\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - 29s 846ms/step - loss: 1.6151 - categorical_accuracy: 0.1176 - val_loss: 1.6075 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-05-2417_26_11.161846/model-00014-1.61507-0.11765-1.60750-0.21000.h5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - 29s 853ms/step - loss: 1.6061 - categorical_accuracy: 0.2843 - val_loss: 1.6075 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-05-2417_26_11.161846/model-00015-1.60612-0.28431-1.60749-0.21000.h5\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - 29s 855ms/step - loss: 1.6095 - categorical_accuracy: 0.2353 - val_loss: 1.6075 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-05-2417_26_11.161846/model-00016-1.60950-0.23529-1.60749-0.21000.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 17/20\n",
      "15/34 [============>.................] - ETA: 13s - loss: 1.6044 - categorical_accuracy: 0.2222"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-ac89e47406d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model_6.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=20, verbose=1, \n\u001b[1;32m      2\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                     validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n\u001b[0m",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    212\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_6.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=20, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_48 (Conv3D)           (None, 20, 72, 72, 64)    5248      \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 20, 72, 72, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_49 (Conv3D)           (None, 20, 72, 72, 32)    55328     \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 20, 72, 72, 32)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_30 (MaxPooling (None, 10, 36, 36, 32)    0         \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 10, 36, 36, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_50 (Conv3D)           (None, 10, 36, 36, 64)    55360     \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 10, 36, 36, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_51 (Conv3D)           (None, 10, 36, 36, 32)    55328     \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 10, 36, 36, 32)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_31 (MaxPooling (None, 5, 18, 18, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 5, 18, 18, 32)     0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 51840)             0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 512)               26542592  \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 26,846,469\n",
      "Trainable params: 26,846,469\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout\n",
    "\n",
    "x = 20 # number of frames\n",
    "y = 72 # image width\n",
    "z = 72 # image height\n",
    "\n",
    "model_7 = Sequential()\n",
    "model_7.add(Conv3D(64, kernel_size=(3, 3, 3), input_shape=(x,y,z,3), padding='same'))\n",
    "model_7.add(Activation('relu'))\n",
    "model_7.add(Conv3D(32, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_7.add(Activation('relu'))\n",
    "model_7.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))\n",
    "model_7.add(Dropout(0.2))\n",
    "\n",
    "model_7.add(Conv3D(64, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_7.add(Activation('relu'))\n",
    "model_7.add(Conv3D(32, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_7.add(Activation('relu'))\n",
    "model_7.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))\n",
    "model_7.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_7.add(Flatten())\n",
    "model_7.add(Dense(512, activation='relu'))\n",
    "model_7.add(Dropout(0.5))\n",
    "\n",
    "model_7.add(Dense(256, activation='relu'))\n",
    "model_7.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model_7.add(Dense(5, activation='softmax'))\n",
    "optimiser = optimizers.Adam(lr=0.001) #write your optimizer\n",
    "model_7.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_7.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "\n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  val ; batch size = 20\n",
      "Source path =  train ; batch size = 20\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/34 [===========================>..] - ETA: 3s - loss: 1.6820 - categorical_accuracy: 0.1953Batch:  34 Index: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 56s 2s/step - loss: 1.6774 - categorical_accuracy: 0.2009 - val_loss: 1.6085 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-05-2417_26_11.161846/model-00001-1.67939-0.19759-1.60851-0.23000.h5\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 11s 316ms/step - loss: 1.6103 - categorical_accuracy: 0.1471 - val_loss: 1.6061 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-05-2417_26_11.161846/model-00002-1.61027-0.14706-1.60615-0.21000.h5\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 11s 317ms/step - loss: 1.6057 - categorical_accuracy: 0.2549 - val_loss: 1.6068 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-05-2417_26_11.161846/model-00003-1.60567-0.25490-1.60681-0.21000.h5\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 11s 317ms/step - loss: 1.6135 - categorical_accuracy: 0.1863 - val_loss: 1.6084 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-05-2417_26_11.161846/model-00004-1.61346-0.18627-1.60838-0.21000.h5\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 11s 317ms/step - loss: 1.6159 - categorical_accuracy: 0.1569 - val_loss: 1.6084 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-05-2417_26_11.161846/model-00005-1.61587-0.15686-1.60845-0.21000.h5\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 11s 317ms/step - loss: 1.6095 - categorical_accuracy: 0.2059 - val_loss: 1.6082 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-05-2417_26_11.161846/model-00006-1.60948-0.20588-1.60817-0.23000.h5\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 11s 317ms/step - loss: 1.6094 - categorical_accuracy: 0.2647 - val_loss: 1.6081 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-05-2417_26_11.161846/model-00007-1.60938-0.26471-1.60813-0.23000.h5\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 11s 316ms/step - loss: 1.6154 - categorical_accuracy: 0.1961 - val_loss: 1.6081 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-05-2417_26_11.161846/model-00008-1.61536-0.19608-1.60809-0.23000.h5\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 11s 317ms/step - loss: 1.6085 - categorical_accuracy: 0.1667 - val_loss: 1.6084 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-05-2417_26_11.161846/model-00009-1.60854-0.16667-1.60839-0.21000.h5\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 11s 318ms/step - loss: 1.6088 - categorical_accuracy: 0.1961 - val_loss: 1.6084 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-05-2417_26_11.161846/model-00010-1.60879-0.19608-1.60836-0.21000.h5\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 11s 318ms/step - loss: 1.6092 - categorical_accuracy: 0.1667 - val_loss: 1.6084 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-05-2417_26_11.161846/model-00011-1.60917-0.16667-1.60844-0.23000.h5\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - 11s 318ms/step - loss: 1.6102 - categorical_accuracy: 0.2059 - val_loss: 1.6084 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-05-2417_26_11.161846/model-00012-1.61022-0.20588-1.60844-0.21000.h5\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 13/20\n",
      "33/34 [============================>.] - ETA: 0s - loss: 1.6113 - categorical_accuracy: 0.2121"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-7cb78717ddbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model_7.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=20, verbose=1, \n\u001b[1;32m      2\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                     validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n\u001b[0m",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    228\u001b[0m                             \u001b[0mval_enqueuer_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                             \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                             workers=0)\n\u001b[0m\u001b[1;32m    231\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                         \u001b[0;31m# No need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m   1467\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1468\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1469\u001b[0;31m             verbose=verbose)\n\u001b[0m\u001b[1;32m   1470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(model, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m    341\u001b[0m                                  \u001b[0;34m'or (x, y). Found: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                                  str(generator_output))\n\u001b[0;32m--> 343\u001b[0;31m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0mouts_per_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[0;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_7.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=20, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_39 (Conv3D)           (None, 15, 100, 100, 64)  5248      \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 15, 100, 100, 64)  0         \n",
      "_________________________________________________________________\n",
      "conv3d_40 (Conv3D)           (None, 15, 100, 100, 32)  55328     \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 15, 100, 100, 32)  0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_20 (MaxPooling (None, 8, 50, 50, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 8, 50, 50, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_41 (Conv3D)           (None, 8, 50, 50, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 8, 50, 50, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_42 (Conv3D)           (None, 8, 50, 50, 32)     55328     \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 8, 50, 50, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_21 (MaxPooling (None, 4, 25, 25, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 4, 25, 25, 32)     0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 80000)             0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 128)               10240128  \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 10,419,973\n",
      "Trainable params: 10,419,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout\n",
    "\n",
    "x = 15 # number of frames\n",
    "y = 100 # image width\n",
    "z = 100 # image height\n",
    "batch_size = 20\n",
    "\n",
    "model_8 = Sequential()\n",
    "model_8.add(Conv3D(64, kernel_size=(3, 3, 3), input_shape=(x,y,z,3), padding='same'))\n",
    "model_8.add(Activation('relu'))\n",
    "model_8.add(Conv3D(32, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_8.add(Activation('relu'))\n",
    "model_8.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))\n",
    "model_8.add(Dropout(0.25))\n",
    "\n",
    "model_8.add(Conv3D(64, kernel_size=(3, 3, 3), input_shape=(x,y,z,3), padding='same'))\n",
    "model_8.add(Activation('relu'))\n",
    "model_8.add(Conv3D(32, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_8.add(Activation('relu'))\n",
    "model_8.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))\n",
    "model_8.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "\n",
    "model_8.add(Flatten())\n",
    "model_8.add(Dense(128, activation='relu'))\n",
    "model_8.add(Dropout(0.5))\n",
    "\n",
    "model_8.add(Dense(64, activation='relu'))\n",
    "model_8.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model_8.add(Dense(5, activation='softmax'))\n",
    "model_8.compile(optimizer=keras.optimizers.Adadelta(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_8.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  val ; batch size = 20\n",
      "Source path =  train ; batch size = 20\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/34 [===================>..........] - ETA: 24s - loss: 1.8092 - categorical_accuracy: 0.1826Batch:  34 Index: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 74s 2s/step - loss: 1.7392 - categorical_accuracy: 0.2001 - val_loss: 1.6082 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-05-2418_47_46.240132/model-00001-1.74730-0.18854-1.60820-0.18000.h5\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 15s 446ms/step - loss: 1.6316 - categorical_accuracy: 0.1569 - val_loss: 1.6086 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-05-2418_47_46.240132/model-00002-1.63158-0.15686-1.60861-0.21000.h5\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 15s 447ms/step - loss: 1.6501 - categorical_accuracy: 0.1667 - val_loss: 1.6092 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-05-2418_47_46.240132/model-00003-1.65011-0.16667-1.60921-0.22000.h5\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.5.\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 15s 445ms/step - loss: 1.6100 - categorical_accuracy: 0.1863 - val_loss: 1.6083 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-05-2418_47_46.240132/model-00004-1.60997-0.18627-1.60828-0.19000.h5\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 15s 445ms/step - loss: 1.6140 - categorical_accuracy: 0.1471 - val_loss: 1.6061 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-05-2418_47_46.240132/model-00005-1.61405-0.14706-1.60610-0.24000.h5\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 15s 445ms/step - loss: 1.5924 - categorical_accuracy: 0.2941 - val_loss: 1.5794 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-05-2418_47_46.240132/model-00006-1.59244-0.29412-1.57935-0.23000.h5\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 15s 445ms/step - loss: 1.7699 - categorical_accuracy: 0.2647 - val_loss: 1.5403 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-05-2418_47_46.240132/model-00007-1.76994-0.26471-1.54034-0.35000.h5\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 15s 444ms/step - loss: 1.5623 - categorical_accuracy: 0.2255 - val_loss: 1.5332 - val_categorical_accuracy: 0.3700\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-05-2418_47_46.240132/model-00008-1.56231-0.22549-1.53323-0.37000.h5\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 15s 444ms/step - loss: 1.5612 - categorical_accuracy: 0.3137 - val_loss: 1.5101 - val_categorical_accuracy: 0.3800\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-05-2418_47_46.240132/model-00009-1.56120-0.31373-1.51013-0.38000.h5\n",
      "Epoch 10/20\n",
      "33/34 [============================>.] - ETA: 0s - loss: 1.5724 - categorical_accuracy: 0.3434"
     ]
    }
   ],
   "source": [
    "model_8.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=20, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_20 (Conv3D)           (None, 25, 60, 60, 8)     656       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 25, 60, 60, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_21 (Conv3D)           (None, 25, 60, 60, 16)    3472      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 25, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_11 (MaxPooling (None, 12, 30, 30, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_22 (Conv3D)           (None, 12, 30, 30, 32)    13856     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 12, 30, 30, 32)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_12 (MaxPooling (None, 6, 15, 15, 32)     0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 43200)             0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               5529728   \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 84)                10836     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 84)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 5)                 425       \n",
      "=================================================================\n",
      "Total params: 5,558,973\n",
      "Trainable params: 5,558,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout\n",
    "\n",
    "x = 25 # number of frames\n",
    "y = 60 # image width\n",
    "z = 60 # image height\n",
    "batch_size = 10\n",
    "\n",
    "model_9 = Sequential()\n",
    "model_9.add(Conv3D(8, kernel_size=(3, 3, 3), input_shape=(x,y,z,3), padding='same'))\n",
    "model_9.add(Activation('relu'))\n",
    "model_9.add(Conv3D(16, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_9.add(Activation('relu'))\n",
    "model_9.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model_9.add(Dropout(0.25))\n",
    "\n",
    "model_9.add(Conv3D(32, kernel_size=(3,3,3), padding='same'))\n",
    "model_9.add(Activation('relu'))\n",
    "model_9.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "model_9.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model_9.add(Flatten())\n",
    "\n",
    "model_9.add(Dense(128, activation='relu'))\n",
    "model_9.add(Dropout(0.5))\n",
    "\n",
    "model_9.add(Dense(84, activation='relu'))\n",
    "model_9.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "\n",
    "# model_9.add(Conv3D(8, kernel_size=(3, 3,3), activation='relu',padding='same',input_shape=(x,y,z,3)))\n",
    "# #model_9.add(MaxPooling3D(pool_size=(2, 2,2),strides=1, padding='same'))\n",
    "# model_9.add(Conv3D(16, kernel_size=(5,5,5), activation='relu', padding='same'))\n",
    "# model_9.add(MaxPooling3D(pool_size=(2, 2,2),strides=1, padding='same'))\n",
    "# model_9.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "\n",
    "# model_9.add(Flatten())\n",
    "# model_9.add(Dense(120, activation='relu'))\n",
    "# model_9.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model_9.add(Dense(5, activation='softmax'))\n",
    "model_9.compile(optimizer=keras.optimizers.Adam(lr=0.001), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_9.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1) # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Source path =  train ; batch size = 10\n",
      "Epoch 1/50val ; batch size = 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/67 [============================>.] - ETA: 1s - loss: 1.6118 - categorical_accuracy: 0.1908Batch:  67 Index: 10\n",
      "66/67 [============================>.] - ETA: 0s - loss: 1.6116 - categorical_accuracy: 0.1955"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 58s 866ms/step - loss: 1.6108 - categorical_accuracy: 0.1975 - val_loss: 1.5990 - val_categorical_accuracy: 0.4100\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.59895, saving model to model_init_2020-05-2513_02_07.143992/model-00001-1.61136-0.19608-1.59895-0.41000.h5\n",
      "Epoch 2/50\n",
      "67/67 [==============================] - 19s 280ms/step - loss: 1.5854 - categorical_accuracy: 0.2239 - val_loss: 1.4992 - val_categorical_accuracy: 0.3600\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.59895 to 1.49922, saving model to model_init_2020-05-2513_02_07.143992/model-00002-1.58539-0.22388-1.49922-0.36000.h5\n",
      "Epoch 3/50\n",
      "67/67 [==============================] - 20s 303ms/step - loss: 1.5691 - categorical_accuracy: 0.3632 - val_loss: 1.5733 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.49922\n",
      "Epoch 4/50\n",
      "67/67 [==============================] - 19s 281ms/step - loss: 1.5937 - categorical_accuracy: 0.2836 - val_loss: 1.5228 - val_categorical_accuracy: 0.4200\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.49922\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 5/50\n",
      "67/67 [==============================] - 19s 286ms/step - loss: 1.5338 - categorical_accuracy: 0.3781 - val_loss: 1.4941 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.49922 to 1.49413, saving model to model_init_2020-05-2513_02_07.143992/model-00005-1.53381-0.37811-1.49413-0.18000.h5\n",
      "Epoch 6/50\n",
      "67/67 [==============================] - 20s 293ms/step - loss: 1.4650 - categorical_accuracy: 0.3532 - val_loss: 1.3688 - val_categorical_accuracy: 0.4200\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.49413 to 1.36875, saving model to model_init_2020-05-2513_02_07.143992/model-00006-1.46502-0.35323-1.36875-0.42000.h5\n",
      "Epoch 7/50\n",
      "67/67 [==============================] - 20s 293ms/step - loss: 1.4805 - categorical_accuracy: 0.3532 - val_loss: 1.4007 - val_categorical_accuracy: 0.4700\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.36875\n",
      "Epoch 8/50\n",
      "67/67 [==============================] - 19s 278ms/step - loss: 1.3421 - categorical_accuracy: 0.4328 - val_loss: 1.2998 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.36875 to 1.29982, saving model to model_init_2020-05-2513_02_07.143992/model-00008-1.34209-0.43284-1.29982-0.45000.h5\n",
      "Epoch 9/50\n",
      "67/67 [==============================] - 19s 284ms/step - loss: 1.3808 - categorical_accuracy: 0.3632 - val_loss: 1.2854 - val_categorical_accuracy: 0.4600\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.29982 to 1.28538, saving model to model_init_2020-05-2513_02_07.143992/model-00009-1.38081-0.36318-1.28538-0.46000.h5\n",
      "Epoch 10/50\n",
      "67/67 [==============================] - 20s 304ms/step - loss: 1.3789 - categorical_accuracy: 0.3980 - val_loss: 1.2296 - val_categorical_accuracy: 0.4900\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.28538 to 1.22958, saving model to model_init_2020-05-2513_02_07.143992/model-00010-1.37886-0.39801-1.22958-0.49000.h5\n",
      "Epoch 11/50\n",
      "67/67 [==============================] - 20s 293ms/step - loss: 1.4481 - categorical_accuracy: 0.3532 - val_loss: 1.2378 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.22958\n",
      "Epoch 12/50\n",
      "67/67 [==============================] - 19s 285ms/step - loss: 1.3516 - categorical_accuracy: 0.4179 - val_loss: 1.1970 - val_categorical_accuracy: 0.4800\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.22958 to 1.19701, saving model to model_init_2020-05-2513_02_07.143992/model-00012-1.35164-0.41791-1.19701-0.48000.h5\n",
      "Epoch 13/50\n",
      "67/67 [==============================] - 19s 282ms/step - loss: 1.3223 - categorical_accuracy: 0.4129 - val_loss: 1.2084 - val_categorical_accuracy: 0.4700\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.19701\n",
      "Epoch 14/50\n",
      "67/67 [==============================] - 21s 307ms/step - loss: 1.2624 - categorical_accuracy: 0.4229 - val_loss: 1.1349 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.19701 to 1.13490, saving model to model_init_2020-05-2513_02_07.143992/model-00014-1.26240-0.42289-1.13490-0.53000.h5\n",
      "Epoch 15/50\n",
      "67/67 [==============================] - 19s 283ms/step - loss: 1.1832 - categorical_accuracy: 0.5075 - val_loss: 1.1314 - val_categorical_accuracy: 0.4800\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.13490 to 1.13140, saving model to model_init_2020-05-2513_02_07.143992/model-00015-1.18322-0.50746-1.13140-0.48000.h5\n",
      "Epoch 16/50\n",
      "67/67 [==============================] - 20s 292ms/step - loss: 1.2279 - categorical_accuracy: 0.4279 - val_loss: 1.0705 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.13140 to 1.07045, saving model to model_init_2020-05-2513_02_07.143992/model-00016-1.22794-0.42786-1.07045-0.56000.h5\n",
      "Epoch 17/50\n",
      "67/67 [==============================] - 20s 293ms/step - loss: 1.1351 - categorical_accuracy: 0.4577 - val_loss: 1.0671 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.07045 to 1.06715, saving model to model_init_2020-05-2513_02_07.143992/model-00017-1.13514-0.45771-1.06715-0.52000.h5\n",
      "Epoch 18/50\n",
      "67/67 [==============================] - 20s 302ms/step - loss: 1.2013 - categorical_accuracy: 0.4776 - val_loss: 1.0588 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.06715 to 1.05883, saving model to model_init_2020-05-2513_02_07.143992/model-00018-1.20133-0.47761-1.05883-0.54000.h5\n",
      "Epoch 19/50\n",
      "67/67 [==============================] - 19s 280ms/step - loss: 1.0742 - categorical_accuracy: 0.5174 - val_loss: 1.1112 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.05883\n",
      "Epoch 20/50\n",
      "67/67 [==============================] - 19s 290ms/step - loss: 1.0227 - categorical_accuracy: 0.5970 - val_loss: 1.0223 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.05883 to 1.02233, saving model to model_init_2020-05-2513_02_07.143992/model-00020-1.02271-0.59701-1.02233-0.57000.h5\n",
      "Epoch 21/50\n",
      "67/67 [==============================] - 19s 291ms/step - loss: 1.1009 - categorical_accuracy: 0.5274 - val_loss: 1.0137 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.02233 to 1.01371, saving model to model_init_2020-05-2513_02_07.143992/model-00021-1.10092-0.52736-1.01371-0.57000.h5\n",
      "Epoch 22/50\n",
      "67/67 [==============================] - 20s 292ms/step - loss: 0.9762 - categorical_accuracy: 0.6318 - val_loss: 0.9856 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.01371 to 0.98557, saving model to model_init_2020-05-2513_02_07.143992/model-00022-0.97615-0.63184-0.98557-0.54000.h5\n",
      "Epoch 23/50\n",
      "67/67 [==============================] - 20s 293ms/step - loss: 0.9075 - categorical_accuracy: 0.6617 - val_loss: 1.0023 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.98557\n",
      "Epoch 24/50\n",
      "67/67 [==============================] - 19s 291ms/step - loss: 0.9859 - categorical_accuracy: 0.5920 - val_loss: 1.0291 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.98557\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 25/50\n",
      "67/67 [==============================] - 19s 289ms/step - loss: 0.8601 - categorical_accuracy: 0.6866 - val_loss: 0.9739 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.98557 to 0.97386, saving model to model_init_2020-05-2513_02_07.143992/model-00025-0.86009-0.68657-0.97386-0.54000.h5\n",
      "Epoch 26/50\n",
      "67/67 [==============================] - 19s 285ms/step - loss: 0.7281 - categorical_accuracy: 0.7164 - val_loss: 1.0038 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.97386\n",
      "Epoch 27/50\n",
      "67/67 [==============================] - 20s 298ms/step - loss: 0.6697 - categorical_accuracy: 0.7264 - val_loss: 1.1215 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.97386\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 28/50\n",
      "67/67 [==============================] - 19s 290ms/step - loss: 0.7349 - categorical_accuracy: 0.7363 - val_loss: 0.9892 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.97386\n",
      "Epoch 29/50\n",
      "67/67 [==============================] - 19s 283ms/step - loss: 0.5959 - categorical_accuracy: 0.7562 - val_loss: 1.0503 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.97386\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 30/50\n",
      "67/67 [==============================] - 20s 297ms/step - loss: 0.6206 - categorical_accuracy: 0.7761 - val_loss: 1.0264 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.97386\n",
      "Epoch 31/50\n",
      "67/67 [==============================] - 19s 286ms/step - loss: 0.6398 - categorical_accuracy: 0.7413 - val_loss: 1.0131 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.97386\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 32/50\n",
      "67/67 [==============================] - 20s 296ms/step - loss: 0.6230 - categorical_accuracy: 0.7512 - val_loss: 1.0143 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.97386\n",
      "Epoch 33/50\n",
      "67/67 [==============================] - 18s 276ms/step - loss: 0.5099 - categorical_accuracy: 0.8109 - val_loss: 1.0289 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.97386\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 34/50\n",
      "67/67 [==============================] - 20s 296ms/step - loss: 0.5622 - categorical_accuracy: 0.7910 - val_loss: 1.0417 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.97386\n",
      "Epoch 35/50\n",
      "67/67 [==============================] - 20s 298ms/step - loss: 0.6092 - categorical_accuracy: 0.7313 - val_loss: 1.0457 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.97386\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 36/50\n",
      "67/67 [==============================] - 19s 288ms/step - loss: 0.5599 - categorical_accuracy: 0.7910 - val_loss: 1.0450 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.97386\n",
      "Epoch 37/50\n",
      "67/67 [==============================] - 19s 290ms/step - loss: 0.5086 - categorical_accuracy: 0.7960 - val_loss: 1.0349 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.97386\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 38/50\n",
      "67/67 [==============================] - 19s 289ms/step - loss: 0.5389 - categorical_accuracy: 0.8308 - val_loss: 1.0385 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.97386\n",
      "Epoch 39/50\n",
      "67/67 [==============================] - 20s 296ms/step - loss: 0.5512 - categorical_accuracy: 0.8259 - val_loss: 1.0381 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.97386\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 40/50\n",
      "67/67 [==============================] - 19s 287ms/step - loss: 0.5629 - categorical_accuracy: 0.7861 - val_loss: 1.0360 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.97386\n",
      "Epoch 41/50\n",
      "67/67 [==============================] - 19s 285ms/step - loss: 0.5476 - categorical_accuracy: 0.7662 - val_loss: 1.0356 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.97386\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 42/50\n",
      "67/67 [==============================] - 20s 304ms/step - loss: 0.5139 - categorical_accuracy: 0.8259 - val_loss: 1.0361 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.97386\n",
      "Epoch 43/50\n",
      "67/67 [==============================] - 20s 292ms/step - loss: 0.5838 - categorical_accuracy: 0.7811 - val_loss: 1.0353 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.97386\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "Epoch 44/50\n",
      "67/67 [==============================] - 19s 291ms/step - loss: 0.5241 - categorical_accuracy: 0.8259 - val_loss: 1.0361 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.97386\n",
      "Epoch 45/50\n",
      "67/67 [==============================] - 20s 299ms/step - loss: 0.6314 - categorical_accuracy: 0.7363 - val_loss: 1.0363 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.97386\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "Epoch 46/50\n",
      "67/67 [==============================] - 19s 288ms/step - loss: 0.4893 - categorical_accuracy: 0.8308 - val_loss: 1.0363 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.97386\n",
      "Epoch 47/50\n",
      "67/67 [==============================] - 19s 280ms/step - loss: 0.6199 - categorical_accuracy: 0.7662 - val_loss: 1.0362 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.97386\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "Epoch 48/50\n",
      "67/67 [==============================] - 20s 295ms/step - loss: 0.6263 - categorical_accuracy: 0.7761 - val_loss: 1.0362 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.97386\n",
      "Epoch 49/50\n",
      "67/67 [==============================] - 19s 290ms/step - loss: 0.5730 - categorical_accuracy: 0.7761 - val_loss: 1.0363 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.97386\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "Epoch 50/50\n",
      "67/67 [==============================] - 19s 288ms/step - loss: 0.5689 - categorical_accuracy: 0.7811 - val_loss: 1.0363 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.97386\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff8c32697b8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_9.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=50, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_26 (Conv3D)           (None, 25, 60, 60, 8)     656       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 25, 60, 60, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_27 (Conv3D)           (None, 25, 60, 60, 16)    3472      \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 25, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_15 (MaxPooling (None, 12, 30, 30, 16)    0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 12, 30, 30, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_28 (Conv3D)           (None, 12, 30, 30, 32)    13856     \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 12, 30, 30, 32)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_16 (MaxPooling (None, 6, 15, 15, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 6, 15, 15, 32)     0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 43200)             0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 128)               5529728   \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 84)                10836     \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 84)                0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 5)                 425       \n",
      "=================================================================\n",
      "Total params: 5,558,973\n",
      "Trainable params: 5,558,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout\n",
    "\n",
    "x = 25 # number of frames\n",
    "y = 60 # image width\n",
    "z = 60 # image height\n",
    "batch_size = 10\n",
    "\n",
    "model_10 = Sequential()\n",
    "model_10.add(Conv3D(8, kernel_size=(3, 3, 3), input_shape=(x,y,z,3), padding='same'))\n",
    "model_10.add(Activation('relu'))\n",
    "model_10.add(Conv3D(16, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_10.add(Activation('relu'))\n",
    "model_10.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model_10.add(Dropout(0.10))\n",
    "\n",
    "model_10.add(Conv3D(32, kernel_size=(3,3,3), padding='same'))\n",
    "model_10.add(Activation('relu'))\n",
    "model_10.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "model_10.add(Dropout(0.10))\n",
    "\n",
    "\n",
    "model_10.add(Flatten())\n",
    "\n",
    "model_10.add(Dense(128, activation='relu'))\n",
    "model_10.add(Dropout(0.25))\n",
    "\n",
    "model_10.add(Dense(84, activation='relu'))\n",
    "model_10.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_10.add(Dense(5, activation='softmax'))\n",
    "model_10.compile(optimizer=keras.optimizers.Adam(lr=0.001), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_10.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1) # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  val ; batch size = 10\n",
      "Source path =  train ; batch size = 10\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/67 [============================>.] - ETA: 1s - loss: 1.5983 - categorical_accuracy: 0.2277Batch:  67 Index: 10\n",
      "66/67 [============================>.] - ETA: 0s - loss: 1.5969 - categorical_accuracy: 0.2288"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 58s 865ms/step - loss: 1.6000 - categorical_accuracy: 0.2254 - val_loss: 1.4374 - val_categorical_accuracy: 0.3400\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.43736, saving model to model_init_2020-05-2513_02_07.143992/model-00001-1.59785-0.22775-1.43736-0.34000.h5\n",
      "Epoch 2/50\n",
      "67/67 [==============================] - 18s 266ms/step - loss: 1.4542 - categorical_accuracy: 0.3532 - val_loss: 1.4505 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.43736\n",
      "Epoch 3/50\n",
      "67/67 [==============================] - 20s 298ms/step - loss: 1.4898 - categorical_accuracy: 0.3483 - val_loss: 1.3363 - val_categorical_accuracy: 0.4200\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.43736 to 1.33628, saving model to model_init_2020-05-2513_02_07.143992/model-00003-1.48978-0.34826-1.33628-0.42000.h5\n",
      "Epoch 4/50\n",
      "67/67 [==============================] - 19s 288ms/step - loss: 1.4790 - categorical_accuracy: 0.3582 - val_loss: 1.2699 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.33628 to 1.26993, saving model to model_init_2020-05-2513_02_07.143992/model-00004-1.47905-0.35821-1.26993-0.44000.h5\n",
      "Epoch 5/50\n",
      "67/67 [==============================] - 21s 307ms/step - loss: 1.2988 - categorical_accuracy: 0.4328 - val_loss: 1.2644 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.26993 to 1.26440, saving model to model_init_2020-05-2513_02_07.143992/model-00005-1.29884-0.43284-1.26440-0.45000.h5\n",
      "Epoch 6/50\n",
      "67/67 [==============================] - 18s 276ms/step - loss: 1.2863 - categorical_accuracy: 0.4876 - val_loss: 1.2013 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.26440 to 1.20128, saving model to model_init_2020-05-2513_02_07.143992/model-00006-1.28626-0.48756-1.20128-0.45000.h5\n",
      "Epoch 7/50\n",
      "67/67 [==============================] - 20s 300ms/step - loss: 1.1539 - categorical_accuracy: 0.5025 - val_loss: 1.1647 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.20128 to 1.16472, saving model to model_init_2020-05-2513_02_07.143992/model-00007-1.15388-0.50249-1.16472-0.45000.h5\n",
      "Epoch 8/50\n",
      "67/67 [==============================] - 20s 295ms/step - loss: 1.1043 - categorical_accuracy: 0.5473 - val_loss: 1.1688 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.16472\n",
      "Epoch 9/50\n",
      "67/67 [==============================] - 19s 291ms/step - loss: 1.0442 - categorical_accuracy: 0.5721 - val_loss: 1.1144 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.16472 to 1.11444, saving model to model_init_2020-05-2513_02_07.143992/model-00009-1.04416-0.57214-1.11444-0.56000.h5\n",
      "Epoch 10/50\n",
      "67/67 [==============================] - 20s 291ms/step - loss: 0.9222 - categorical_accuracy: 0.6169 - val_loss: 1.0073 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.11444 to 1.00729, saving model to model_init_2020-05-2513_02_07.143992/model-00010-0.92218-0.61692-1.00729-0.61000.h5\n",
      "Epoch 11/50\n",
      "67/67 [==============================] - 20s 292ms/step - loss: 1.0677 - categorical_accuracy: 0.5274 - val_loss: 1.0220 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.00729\n",
      "Epoch 12/50\n",
      "67/67 [==============================] - 20s 294ms/step - loss: 0.8404 - categorical_accuracy: 0.6368 - val_loss: 0.9738 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.00729 to 0.97380, saving model to model_init_2020-05-2513_02_07.143992/model-00012-0.84041-0.63682-0.97380-0.59000.h5\n",
      "Epoch 13/50\n",
      "67/67 [==============================] - 20s 297ms/step - loss: 0.8306 - categorical_accuracy: 0.6567 - val_loss: 0.9884 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.97380\n",
      "Epoch 14/50\n",
      "67/67 [==============================] - 18s 275ms/step - loss: 0.8641 - categorical_accuracy: 0.6766 - val_loss: 0.8620 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.97380 to 0.86204, saving model to model_init_2020-05-2513_02_07.143992/model-00014-0.86409-0.67662-0.86204-0.65000.h5\n",
      "Epoch 15/50\n",
      "67/67 [==============================] - 20s 294ms/step - loss: 0.6825 - categorical_accuracy: 0.7363 - val_loss: 0.9754 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.86204\n",
      "Epoch 16/50\n",
      "67/67 [==============================] - 19s 286ms/step - loss: 0.5952 - categorical_accuracy: 0.7114 - val_loss: 0.8300 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.86204 to 0.83002, saving model to model_init_2020-05-2513_02_07.143992/model-00016-0.59517-0.71144-0.83002-0.64000.h5\n",
      "Epoch 17/50\n",
      "67/67 [==============================] - 19s 290ms/step - loss: 0.5677 - categorical_accuracy: 0.8060 - val_loss: 0.9211 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.83002\n",
      "Epoch 18/50\n",
      "67/67 [==============================] - 20s 304ms/step - loss: 0.5349 - categorical_accuracy: 0.7861 - val_loss: 0.8657 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.83002\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 19/50\n",
      "67/67 [==============================] - 20s 293ms/step - loss: 0.3591 - categorical_accuracy: 0.8458 - val_loss: 0.8860 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.83002\n",
      "Epoch 20/50\n",
      "67/67 [==============================] - 19s 278ms/step - loss: 0.3814 - categorical_accuracy: 0.8657 - val_loss: 0.8358 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.83002\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 21/50\n",
      "67/67 [==============================] - 19s 288ms/step - loss: 0.3300 - categorical_accuracy: 0.8706 - val_loss: 0.8598 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.83002\n",
      "Epoch 22/50\n",
      "67/67 [==============================] - 19s 286ms/step - loss: 0.2642 - categorical_accuracy: 0.9154 - val_loss: 0.7948 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.83002 to 0.79476, saving model to model_init_2020-05-2513_02_07.143992/model-00022-0.26425-0.91542-0.79476-0.71000.h5\n",
      "Epoch 23/50\n",
      "67/67 [==============================] - 19s 290ms/step - loss: 0.2090 - categorical_accuracy: 0.9204 - val_loss: 0.7924 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.79476 to 0.79240, saving model to model_init_2020-05-2513_02_07.143992/model-00023-0.20904-0.92040-0.79240-0.74000.h5\n",
      "Epoch 24/50\n",
      "67/67 [==============================] - 19s 287ms/step - loss: 0.2099 - categorical_accuracy: 0.9254 - val_loss: 0.8175 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.79240\n",
      "Epoch 25/50\n",
      "67/67 [==============================] - 19s 287ms/step - loss: 0.2063 - categorical_accuracy: 0.9353 - val_loss: 0.8293 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.79240\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 26/50\n",
      "67/67 [==============================] - 20s 292ms/step - loss: 0.1628 - categorical_accuracy: 0.9602 - val_loss: 0.8408 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.79240\n",
      "Epoch 27/50\n",
      "67/67 [==============================] - 20s 295ms/step - loss: 0.1702 - categorical_accuracy: 0.9652 - val_loss: 0.8201 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.79240\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 28/50\n",
      "67/67 [==============================] - 19s 278ms/step - loss: 0.1206 - categorical_accuracy: 0.9552 - val_loss: 0.8337 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.79240\n",
      "Epoch 29/50\n",
      "67/67 [==============================] - 19s 287ms/step - loss: 0.1286 - categorical_accuracy: 0.9652 - val_loss: 0.8316 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.79240\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 30/50\n",
      "67/67 [==============================] - 20s 297ms/step - loss: 0.0892 - categorical_accuracy: 0.9801 - val_loss: 0.8292 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.79240\n",
      "Epoch 31/50\n",
      "67/67 [==============================] - 20s 294ms/step - loss: 0.1031 - categorical_accuracy: 0.9552 - val_loss: 0.8288 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.79240\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 32/50\n",
      "67/67 [==============================] - 20s 295ms/step - loss: 0.0706 - categorical_accuracy: 0.9801 - val_loss: 0.8320 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.79240\n",
      "Epoch 33/50\n",
      "67/67 [==============================] - 20s 297ms/step - loss: 0.1289 - categorical_accuracy: 0.9652 - val_loss: 0.8352 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.79240\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 34/50\n",
      "67/67 [==============================] - 19s 277ms/step - loss: 0.1199 - categorical_accuracy: 0.9751 - val_loss: 0.8359 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.79240\n",
      "Epoch 35/50\n",
      "67/67 [==============================] - 20s 302ms/step - loss: 0.0981 - categorical_accuracy: 0.9701 - val_loss: 0.8369 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.79240\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 36/50\n",
      "67/67 [==============================] - 18s 275ms/step - loss: 0.0975 - categorical_accuracy: 0.9801 - val_loss: 0.8378 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.79240\n",
      "Epoch 37/50\n",
      "67/67 [==============================] - 19s 286ms/step - loss: 0.1268 - categorical_accuracy: 0.9652 - val_loss: 0.8379 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.79240\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 38/50\n",
      "67/67 [==============================] - 20s 299ms/step - loss: 0.1143 - categorical_accuracy: 0.9701 - val_loss: 0.8384 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.79240\n",
      "Epoch 39/50\n",
      "67/67 [==============================] - 20s 291ms/step - loss: 0.1162 - categorical_accuracy: 0.9502 - val_loss: 0.8382 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.79240\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 40/50\n",
      "67/67 [==============================] - 20s 292ms/step - loss: 0.1332 - categorical_accuracy: 0.9552 - val_loss: 0.8389 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.79240\n",
      "Epoch 41/50\n",
      "67/67 [==============================] - 19s 290ms/step - loss: 0.1301 - categorical_accuracy: 0.9652 - val_loss: 0.8383 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.79240\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "Epoch 42/50\n",
      "67/67 [==============================] - 20s 291ms/step - loss: 0.0977 - categorical_accuracy: 0.9652 - val_loss: 0.8385 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.79240\n",
      "Epoch 43/50\n",
      "67/67 [==============================] - 20s 292ms/step - loss: 0.1292 - categorical_accuracy: 0.9602 - val_loss: 0.8385 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.79240\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "Epoch 44/50\n",
      "67/67 [==============================] - 19s 285ms/step - loss: 0.0989 - categorical_accuracy: 0.9701 - val_loss: 0.8386 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.79240\n",
      "Epoch 45/50\n",
      "67/67 [==============================] - 19s 282ms/step - loss: 0.1347 - categorical_accuracy: 0.9502 - val_loss: 0.8387 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.79240\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "Epoch 46/50\n",
      "67/67 [==============================] - 20s 302ms/step - loss: 0.1077 - categorical_accuracy: 0.9801 - val_loss: 0.8387 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.79240\n",
      "Epoch 47/50\n",
      "67/67 [==============================] - 19s 288ms/step - loss: 0.0987 - categorical_accuracy: 0.9652 - val_loss: 0.8387 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.79240\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "Epoch 48/50\n",
      "67/67 [==============================] - 20s 291ms/step - loss: 0.0851 - categorical_accuracy: 0.9900 - val_loss: 0.8387 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.79240\n",
      "Epoch 49/50\n",
      "67/67 [==============================] - 19s 284ms/step - loss: 0.0980 - categorical_accuracy: 0.9602 - val_loss: 0.8387 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.79240\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
      "Epoch 50/50\n",
      "67/67 [==============================] - 19s 284ms/step - loss: 0.1119 - categorical_accuracy: 0.9552 - val_loss: 0.8387 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.79240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff8c1d191d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_10.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=50, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model is overfitting. Train accuracy = 95% and validation accuracy = 72% till here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding dropouts and L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_17 (Conv3D)           (None, 30, 60, 60, 8)     656       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 30, 60, 60, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_18 (Conv3D)           (None, 30, 60, 60, 16)    3472      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 30, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_10 (MaxPooling (None, 15, 30, 30, 16)    0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 15, 30, 30, 16)    0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 216000)            0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 48)                10368048  \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 5)                 245       \n",
      "=================================================================\n",
      "Total params: 10,372,421\n",
      "Trainable params: 10,372,421\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout\n",
    "from keras.regularizers import l2\n",
    "\n",
    "x = 30 # number of frames\n",
    "y = 60 # image width\n",
    "z = 60 # image height\n",
    "batch_size = 10\n",
    "\n",
    "model_11 = Sequential()\n",
    "model_11.add(Conv3D(8, kernel_size=(3, 3, 3), input_shape=(x,y,z,3), padding='same'))\n",
    "model_11.add(Activation('relu'))\n",
    "model_11.add(Conv3D(16, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_11.add(Activation('relu'))\n",
    "model_11.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model_11.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model_11.add(Flatten())\n",
    "\n",
    "model_11.add(Dense(48, activation='relu',kernel_regularizer=l2(0.001)))\n",
    "model_11.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model_11.add(Dense(5, activation='softmax'))\n",
    "model_11.compile(optimizer=keras.optimizers.Adam(lr=0.001), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_11.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1) # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  val ; batch size = 10\n",
      "Source path =  trainEpoch 1/30\n",
      " ; batch size = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/67 [============================>.] - ETA: 2s - loss: 2.0551 - categorical_accuracy: 0.2262Batch:  67 Index: 10\n",
      "66/67 [============================>.] - ETA: 1s - loss: 2.0493 - categorical_accuracy: 0.2288"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 70s 1s/step - loss: 2.0316 - categorical_accuracy: 0.2402 - val_loss: 1.7201 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.72013, saving model to model_init_2020-05-2514_50_20.061003/model-00001-2.04387-0.23228-1.72013-0.31000.h5\n",
      "Epoch 2/30\n",
      "67/67 [==============================] - 22s 332ms/step - loss: 1.6490 - categorical_accuracy: 0.3532 - val_loss: 1.6093 - val_categorical_accuracy: 0.4100\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.72013 to 1.60929, saving model to model_init_2020-05-2514_50_20.061003/model-00002-1.64895-0.35323-1.60929-0.41000.h5\n",
      "Epoch 3/30\n",
      "67/67 [==============================] - 23s 340ms/step - loss: 1.6568 - categorical_accuracy: 0.4030 - val_loss: 1.5745 - val_categorical_accuracy: 0.4900\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.60929 to 1.57450, saving model to model_init_2020-05-2514_50_20.061003/model-00003-1.65677-0.40299-1.57450-0.49000.h5\n",
      "Epoch 4/30\n",
      "67/67 [==============================] - 24s 357ms/step - loss: 1.5891 - categorical_accuracy: 0.3433 - val_loss: 1.4550 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.57450 to 1.45496, saving model to model_init_2020-05-2514_50_20.061003/model-00004-1.58914-0.34328-1.45496-0.60000.h5\n",
      "Epoch 5/30\n",
      "67/67 [==============================] - 23s 351ms/step - loss: 1.5636 - categorical_accuracy: 0.4726 - val_loss: 1.5519 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.45496\n",
      "Epoch 6/30\n",
      "67/67 [==============================] - 23s 338ms/step - loss: 1.5923 - categorical_accuracy: 0.4179 - val_loss: 1.4765 - val_categorical_accuracy: 0.4800\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.45496\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 7/30\n",
      "67/67 [==============================] - 23s 344ms/step - loss: 1.4130 - categorical_accuracy: 0.4826 - val_loss: 1.4066 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.45496 to 1.40659, saving model to model_init_2020-05-2514_50_20.061003/model-00007-1.41302-0.48259-1.40659-0.54000.h5\n",
      "Epoch 8/30\n",
      "67/67 [==============================] - 22s 321ms/step - loss: 1.3438 - categorical_accuracy: 0.4975 - val_loss: 1.3207 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.40659 to 1.32065, saving model to model_init_2020-05-2514_50_20.061003/model-00008-1.34384-0.49751-1.32065-0.60000.h5\n",
      "Epoch 9/30\n",
      "67/67 [==============================] - 24s 364ms/step - loss: 1.2190 - categorical_accuracy: 0.6269 - val_loss: 1.3569 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.32065\n",
      "Epoch 10/30\n",
      "67/67 [==============================] - 23s 336ms/step - loss: 1.2073 - categorical_accuracy: 0.5672 - val_loss: 1.3515 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.32065\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 11/30\n",
      "67/67 [==============================] - 23s 349ms/step - loss: 1.1109 - categorical_accuracy: 0.6667 - val_loss: 1.3028 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.32065 to 1.30285, saving model to model_init_2020-05-2514_50_20.061003/model-00011-1.11092-0.66667-1.30285-0.62000.h5\n",
      "Epoch 12/30\n",
      "67/67 [==============================] - 23s 345ms/step - loss: 1.0176 - categorical_accuracy: 0.6617 - val_loss: 1.2720 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.30285 to 1.27198, saving model to model_init_2020-05-2514_50_20.061003/model-00012-1.01763-0.66169-1.27198-0.62000.h5\n",
      "Epoch 13/30\n",
      "67/67 [==============================] - 23s 347ms/step - loss: 0.9883 - categorical_accuracy: 0.6965 - val_loss: 1.2266 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.27198 to 1.22663, saving model to model_init_2020-05-2514_50_20.061003/model-00013-0.98831-0.69652-1.22663-0.62000.h5\n",
      "Epoch 14/30\n",
      "67/67 [==============================] - 23s 341ms/step - loss: 0.9148 - categorical_accuracy: 0.7015 - val_loss: 1.2106 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.22663 to 1.21059, saving model to model_init_2020-05-2514_50_20.061003/model-00014-0.91484-0.70149-1.21059-0.65000.h5\n",
      "Epoch 15/30\n",
      "67/67 [==============================] - 22s 333ms/step - loss: 0.9317 - categorical_accuracy: 0.7214 - val_loss: 1.3006 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.21059\n",
      "Epoch 16/30\n",
      "67/67 [==============================] - 23s 340ms/step - loss: 1.0237 - categorical_accuracy: 0.6567 - val_loss: 1.2200 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.21059\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 17/30\n",
      "67/67 [==============================] - 24s 359ms/step - loss: 0.8094 - categorical_accuracy: 0.7811 - val_loss: 1.1957 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.21059 to 1.19568, saving model to model_init_2020-05-2514_50_20.061003/model-00017-0.80939-0.78109-1.19568-0.62000.h5\n",
      "Epoch 18/30\n",
      "67/67 [==============================] - 23s 349ms/step - loss: 0.7682 - categorical_accuracy: 0.8458 - val_loss: 1.3058 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.19568\n",
      "Epoch 19/30\n",
      "67/67 [==============================] - 24s 356ms/step - loss: 0.8137 - categorical_accuracy: 0.7612 - val_loss: 1.1471 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.19568 to 1.14708, saving model to model_init_2020-05-2514_50_20.061003/model-00019-0.81373-0.76119-1.14708-0.66000.h5\n",
      "Epoch 20/30\n",
      "67/67 [==============================] - 22s 328ms/step - loss: 0.8458 - categorical_accuracy: 0.7413 - val_loss: 1.1960 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.14708\n",
      "Epoch 21/30\n",
      "67/67 [==============================] - 24s 354ms/step - loss: 0.7949 - categorical_accuracy: 0.8209 - val_loss: 1.2062 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.14708\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 22/30\n",
      "67/67 [==============================] - 23s 337ms/step - loss: 0.7829 - categorical_accuracy: 0.7960 - val_loss: 1.1959 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.14708\n",
      "Epoch 23/30\n",
      "67/67 [==============================] - 23s 342ms/step - loss: 0.6566 - categorical_accuracy: 0.8458 - val_loss: 1.1748 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.14708\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 24/30\n",
      "67/67 [==============================] - 24s 353ms/step - loss: 0.7021 - categorical_accuracy: 0.7960 - val_loss: 1.1911 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.14708\n",
      "Epoch 25/30\n",
      "67/67 [==============================] - 24s 362ms/step - loss: 0.6797 - categorical_accuracy: 0.8159 - val_loss: 1.1885 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.14708\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 26/30\n",
      "67/67 [==============================] - 23s 339ms/step - loss: 0.6424 - categorical_accuracy: 0.8358 - val_loss: 1.1703 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.14708\n",
      "Epoch 27/30\n",
      "67/67 [==============================] - 22s 331ms/step - loss: 0.6382 - categorical_accuracy: 0.8259 - val_loss: 1.1627 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.14708\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 28/30\n",
      "67/67 [==============================] - 23s 346ms/step - loss: 0.6955 - categorical_accuracy: 0.7761 - val_loss: 1.1582 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.14708\n",
      "Epoch 29/30\n",
      "67/67 [==============================] - 23s 338ms/step - loss: 0.6479 - categorical_accuracy: 0.8358 - val_loss: 1.1633 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.14708\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 30/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 22s 335ms/step - loss: 0.6320 - categorical_accuracy: 0.8458 - val_loss: 1.1596 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.14708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1f1d7be518>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_11.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=30, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance improvement seen after regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN + RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, GRU, Flatten, TimeDistributed, Dropout, BatchNormalization, Activation, Reshape, GlobalAveragePooling2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "##### 2D-CNN and RNN model\n",
    "# input size (batch_size=20, x=15, y=120, z=120, channel=3)\n",
    "x=30\n",
    "y=120\n",
    "z=120\n",
    "batch_size=10\n",
    "# input layer\n",
    "input_tensor = Input(shape=(x, y, z, 3))\n",
    "\n",
    "# Get pre-trained model: vgg16 \n",
    "vgg_layers = VGG16(weights='imagenet', include_top=False)\n",
    "    # freeze the layers in base model\n",
    "for layer in vgg_layers.layers:\n",
    "    layer.trainable = False\n",
    "    # create a VGG model selecting all layers   \n",
    "vgg_model = Model(inputs=vgg_layers.input, outputs=vgg_layers.output)\n",
    "\n",
    "\n",
    "# Adding Time Distributed wrapper on top of resnet model and passing the input tensor\n",
    "time_distributed_layer= TimeDistributed(vgg_model)(input_tensor)\n",
    "\n",
    "# Average pooling layer\n",
    "avg_pool_layer= TimeDistributed(GlobalAveragePooling2D())(time_distributed_layer)\n",
    "\n",
    "# Flatten before connecting to GRU\n",
    "flatten_layer = TimeDistributed(Flatten())(avg_pool_layer)\n",
    "flatten_dropped_out=Dropout(0.5)(flatten_layer)\n",
    "\n",
    "# reshape the output of time distributed layer to be fed into LSTM or GRU\n",
    "tensor_size=np.prod(time_distributed_layer.get_shape().as_list()[2:]) \n",
    "reshape_layer = Reshape(target_shape=(x,tensor_size))(time_distributed_layer)\n",
    "         \n",
    "# GRU layer\n",
    "gru_out = GRU(256, return_sequences=False, dropout=0.5)(flatten_dropped_out)\n",
    "\n",
    "# Fully connected Dense Layer\n",
    "fc_out = Dense(256, activation=\"relu\")(gru_out)\n",
    "fc_dropped_out=Dropout(0.5)(fc_out)\n",
    "bn_layer=BatchNormalization()(fc_dropped_out)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(5, activation='softmax')(gru_out)\n",
    "\n",
    "# final Model\n",
    "model2 = Model(inputs=input_tensor, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 30, 120, 120, 3)   0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 30, 3, 3, 512)     14714688  \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 30, 512)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 30, 512)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30, 512)           0         \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 256)               590592    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 15,306,565\n",
      "Trainable params: 591,877\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = 'adam'\n",
    "model2.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 20\n"
     ]
    }
   ],
   "source": [
    "# if not experiment use experiment=0\n",
    "experiment=0\n",
    "\n",
    "if experiment!=0 and batch_size>experiment:\n",
    "    batch_size=experiment\n",
    "\n",
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'train'\n",
    "val_path = 'val'\n",
    "\n",
    "if experiment!=0:\n",
    "    num_train_sequences = len(train_doc[:experiment])\n",
    "    print('# training sequences =', num_train_sequences)\n",
    "    num_val_sequences = len(val_doc[:experiment])\n",
    "    print('# validation sequences =', num_val_sequences)\n",
    "else:\n",
    "    num_train_sequences = len(train_doc)\n",
    "    print('# training sequences =', num_train_sequences)\n",
    "    num_val_sequences = len(val_doc)\n",
    "    print('# validation sequences =', num_val_sequences)\n",
    "    \n",
    "num_epochs = 20\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=2, cooldown=1)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  val ; batch size = 10\n",
      "Source path =  train ; batch size = 10\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/67 [============================>.] - ETA: 2s - loss: 1.6756 - categorical_accuracy: 0.2769Batch:  67 Index: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 92s 1s/step - loss: 1.6722 - categorical_accuracy: 0.2781 - val_loss: 1.4254 - val_categorical_accuracy: 0.4300\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.42540, saving model to model_init_2020-05-2517_43_24.798127/model-00001-1.67369-0.27753-1.42540-0.43000.h5\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 31s 462ms/step - loss: 1.5249 - categorical_accuracy: 0.3383 - val_loss: 1.2701 - val_categorical_accuracy: 0.4800\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.42540 to 1.27005, saving model to model_init_2020-05-2517_43_24.798127/model-00002-1.52490-0.33831-1.27005-0.48000.h5\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 31s 456ms/step - loss: 1.5172 - categorical_accuracy: 0.3333 - val_loss: 1.3501 - val_categorical_accuracy: 0.4100\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.27005\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 31s 460ms/step - loss: 1.4250 - categorical_accuracy: 0.4179 - val_loss: 1.2489 - val_categorical_accuracy: 0.4100\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.27005 to 1.24887, saving model to model_init_2020-05-2517_43_24.798127/model-00004-1.42503-0.41791-1.24887-0.41000.h5\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 31s 459ms/step - loss: 1.3301 - categorical_accuracy: 0.4527 - val_loss: 1.3386 - val_categorical_accuracy: 0.4600\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.24887\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 31s 465ms/step - loss: 1.3723 - categorical_accuracy: 0.4279 - val_loss: 1.1606 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.24887 to 1.16057, saving model to model_init_2020-05-2517_43_24.798127/model-00006-1.37229-0.42786-1.16057-0.50000.h5\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 31s 468ms/step - loss: 1.2427 - categorical_accuracy: 0.4726 - val_loss: 1.1332 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.16057 to 1.13316, saving model to model_init_2020-05-2517_43_24.798127/model-00007-1.24272-0.47264-1.13316-0.56000.h5\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 30s 454ms/step - loss: 1.1224 - categorical_accuracy: 0.5721 - val_loss: 1.1502 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.13316\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 30s 451ms/step - loss: 1.2308 - categorical_accuracy: 0.4925 - val_loss: 1.0506 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.13316 to 1.05056, saving model to model_init_2020-05-2517_43_24.798127/model-00009-1.23075-0.49254-1.05056-0.60000.h5\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 30s 452ms/step - loss: 1.2014 - categorical_accuracy: 0.4677 - val_loss: 1.1550 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.05056\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 30s 449ms/step - loss: 1.1633 - categorical_accuracy: 0.5025 - val_loss: 1.2396 - val_categorical_accuracy: 0.4300\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.05056\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 30s 452ms/step - loss: 1.0969 - categorical_accuracy: 0.5423 - val_loss: 1.0376 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.05056 to 1.03763, saving model to model_init_2020-05-2517_43_24.798127/model-00012-1.09691-0.54229-1.03763-0.61000.h5\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 31s 463ms/step - loss: 1.1076 - categorical_accuracy: 0.5622 - val_loss: 1.0632 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.03763\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 31s 466ms/step - loss: 1.0190 - categorical_accuracy: 0.5920 - val_loss: 1.0566 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.03763\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 31s 457ms/step - loss: 1.0195 - categorical_accuracy: 0.5721 - val_loss: 1.0563 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.03763\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 30s 445ms/step - loss: 1.0008 - categorical_accuracy: 0.6169 - val_loss: 1.0440 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.03763\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 31s 465ms/step - loss: 1.0349 - categorical_accuracy: 0.6070 - val_loss: 1.0386 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.03763\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 30s 441ms/step - loss: 1.0545 - categorical_accuracy: 0.5771 - val_loss: 1.0396 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.03763\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 31s 457ms/step - loss: 0.9782 - categorical_accuracy: 0.6119 - val_loss: 1.0395 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.03763\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 31s 456ms/step - loss: 0.9804 - categorical_accuracy: 0.6219 - val_loss: 1.0386 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.03763\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0bb5eea438>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment No.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 30\n",
    "y = 120\n",
    "z = 120\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(0,x)]  #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    temp = imresize(image,(120,120))\n",
    "                    temp = temp/127.5-1\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2]) #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            print(\"Batch: \",num_batches+1,\"Index:\", batch_size)\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    temp = imresize(image,(120,120))\n",
    "                    temp = temp/127.5-1 #Normalize data\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0])\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1])\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2])\n",
    "                   \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 20\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'train'\n",
    "val_path = 'val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 20 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, GRU, Flatten, TimeDistributed, Dropout, BatchNormalization, Activation, Reshape, GlobalAveragePooling2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "##### 2D-CNN and RNN model\n",
    "# input size (batch_size=20, x=15, y=120, z=120, channel=3)\n",
    "\n",
    "# input layer\n",
    "input_tensor = Input(shape=(x, y, z, 3))\n",
    "\n",
    "# Get pre-trained model: vgg16 \n",
    "vgg_layers = VGG16(weights='imagenet', include_top=False)\n",
    "    # freeze the layers in base model\n",
    "for layer in vgg_layers.layers:\n",
    "    layer.trainable = False\n",
    "    # create a VGG model selecting all layers   \n",
    "vgg_model = Model(inputs=vgg_layers.input, outputs=vgg_layers.output)\n",
    "\n",
    "\n",
    "# Adding Time Distributed wrapper on top of resnet model and passing the input tensor\n",
    "time_distributed_layer= TimeDistributed(vgg_model)(input_tensor)\n",
    "\n",
    "# Average pooling layer\n",
    "avg_pool_layer= TimeDistributed(GlobalAveragePooling2D())(time_distributed_layer)\n",
    "\n",
    "# Flatten before connecting to GRU\n",
    "flatten_layer = TimeDistributed(Flatten())(avg_pool_layer)\n",
    "flatten_dropped_out=Dropout(0.5)(flatten_layer)\n",
    "\n",
    "# reshape the output of time distributed layer to be fed into LSTM or GRU\n",
    "tensor_size=np.prod(time_distributed_layer.get_shape().as_list()[2:]) \n",
    "reshape_layer = Reshape(target_shape=(x,tensor_size))(time_distributed_layer)\n",
    "         \n",
    "# GRU layer\n",
    "gru_out = GRU(256, return_sequences=False, dropout=0.5)(flatten_dropped_out)\n",
    "\n",
    "# Fully connected Dense Layer\n",
    "fc_out = Dense(256, activation=\"relu\")(gru_out)\n",
    "fc_dropped_out=Dropout(0.5)(fc_out)\n",
    "bn_layer=BatchNormalization()(fc_dropped_out)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(5, activation='softmax')(gru_out)\n",
    "\n",
    "# final Model\n",
    "model2 = Model(inputs=input_tensor, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 30, 120, 120, 3)   0         \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 30, 3, 3, 512)     14714688  \n",
      "_________________________________________________________________\n",
      "time_distributed_14 (TimeDis (None, 30, 512)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_15 (TimeDis (None, 30, 512)           0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 30, 512)           0         \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 256)               590592    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 15,306,565\n",
      "Trainable params: 591,877\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = 'adam'\n",
    "model2.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 20\n"
     ]
    }
   ],
   "source": [
    "# if not experiment use experiment=0\n",
    "experiment=0\n",
    "\n",
    "if experiment!=0 and batch_size>experiment:\n",
    "    batch_size=experiment\n",
    "\n",
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'train'\n",
    "val_path = 'val'\n",
    "\n",
    "if experiment!=0:\n",
    "    num_train_sequences = len(train_doc[:experiment])\n",
    "    print('# training sequences =', num_train_sequences)\n",
    "    num_val_sequences = len(val_doc[:experiment])\n",
    "    print('# validation sequences =', num_val_sequences)\n",
    "else:\n",
    "    num_train_sequences = len(train_doc)\n",
    "    print('# training sequences =', num_train_sequences)\n",
    "    num_val_sequences = len(val_doc)\n",
    "    print('# validation sequences =', num_val_sequences)\n",
    "    \n",
    "num_epochs = 20\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=2, cooldown=1)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path = Source path =  train ; batch size = 10\n",
      "Epoch 1/20\n",
      " val ; batch size = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/67 [============================>.] - ETA: 2s - loss: 1.5862 - categorical_accuracy: 0.3277Batch:  67 Index: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 88s 1s/step - loss: 1.5724 - categorical_accuracy: 0.3342 - val_loss: 1.1594 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.15944, saving model to model_init_2020-05-2518_05_33.697998/model-00001-1.58154-0.32730-1.15944-0.57000.h5\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 31s 456ms/step - loss: 1.2952 - categorical_accuracy: 0.4826 - val_loss: 1.1561 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.15944 to 1.15608, saving model to model_init_2020-05-2518_05_33.697998/model-00002-1.29516-0.48259-1.15608-0.54000.h5\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 30s 441ms/step - loss: 1.3295 - categorical_accuracy: 0.4428 - val_loss: 1.0976 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.15608 to 1.09757, saving model to model_init_2020-05-2518_05_33.697998/model-00003-1.32949-0.44279-1.09757-0.54000.h5\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 29s 430ms/step - loss: 1.2839 - categorical_accuracy: 0.4876 - val_loss: 1.0870 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.09757 to 1.08705, saving model to model_init_2020-05-2518_05_33.697998/model-00004-1.28389-0.48756-1.08705-0.54000.h5\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 30s 450ms/step - loss: 1.2160 - categorical_accuracy: 0.5274 - val_loss: 1.0678 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.08705 to 1.06776, saving model to model_init_2020-05-2518_05_33.697998/model-00005-1.21599-0.52736-1.06776-0.51000.h5\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 31s 464ms/step - loss: 1.2297 - categorical_accuracy: 0.5075 - val_loss: 0.9573 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.06776 to 0.95734, saving model to model_init_2020-05-2518_05_33.697998/model-00006-1.22968-0.50746-0.95734-0.64000.h5\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 29s 437ms/step - loss: 1.1023 - categorical_accuracy: 0.5572 - val_loss: 0.9038 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.95734 to 0.90383, saving model to model_init_2020-05-2518_05_33.697998/model-00007-1.10232-0.55721-0.90383-0.66000.h5\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 29s 434ms/step - loss: 1.1356 - categorical_accuracy: 0.5323 - val_loss: 0.9968 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.90383\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 29s 430ms/step - loss: 1.0250 - categorical_accuracy: 0.5970 - val_loss: 0.8728 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.90383 to 0.87277, saving model to model_init_2020-05-2518_05_33.697998/model-00009-1.02501-0.59701-0.87277-0.65000.h5\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 29s 440ms/step - loss: 0.9636 - categorical_accuracy: 0.6368 - val_loss: 0.8470 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.87277 to 0.84704, saving model to model_init_2020-05-2518_05_33.697998/model-00010-0.96359-0.63682-0.84704-0.68000.h5\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 29s 440ms/step - loss: 1.1705 - categorical_accuracy: 0.5274 - val_loss: 0.9290 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.84704\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 29s 436ms/step - loss: 0.9585 - categorical_accuracy: 0.6070 - val_loss: 0.8426 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.84704 to 0.84260, saving model to model_init_2020-05-2518_05_33.697998/model-00012-0.95850-0.60697-0.84260-0.67000.h5\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 30s 450ms/step - loss: 0.9182 - categorical_accuracy: 0.5721 - val_loss: 0.9533 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.84260\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 29s 440ms/step - loss: 0.8675 - categorical_accuracy: 0.6667 - val_loss: 0.9088 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.84260\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 30s 455ms/step - loss: 0.9084 - categorical_accuracy: 0.6418 - val_loss: 0.7853 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.84260 to 0.78535, saving model to model_init_2020-05-2518_05_33.697998/model-00015-0.90835-0.64179-0.78535-0.69000.h5\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 31s 463ms/step - loss: 0.9091 - categorical_accuracy: 0.6418 - val_loss: 0.7858 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.78535\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 30s 445ms/step - loss: 0.7535 - categorical_accuracy: 0.7065 - val_loss: 0.8351 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.78535\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 29s 438ms/step - loss: 0.7418 - categorical_accuracy: 0.7164 - val_loss: 0.7829 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.78535 to 0.78289, saving model to model_init_2020-05-2518_05_33.697998/model-00018-0.74184-0.71642-0.78289-0.69000.h5\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 31s 457ms/step - loss: 0.8716 - categorical_accuracy: 0.6716 - val_loss: 0.7758 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.78289 to 0.77579, saving model to model_init_2020-05-2518_05_33.697998/model-00019-0.87157-0.67164-0.77579-0.67000.h5\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 30s 445ms/step - loss: 0.8732 - categorical_accuracy: 0.6468 - val_loss: 0.7674 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.77579 to 0.76738, saving model to model_init_2020-05-2518_05_33.697998/model-00020-0.87317-0.64677-0.76738-0.69000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0b4f379780>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
